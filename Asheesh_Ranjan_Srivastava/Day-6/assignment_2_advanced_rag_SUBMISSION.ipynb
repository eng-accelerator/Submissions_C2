{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Advanced RAG Techniques\n",
    "## Day 6 Session 2 - Advanced RAG Fundamentals\n",
    "\n",
    "**OBJECTIVE:** Implement advanced RAG techniques including postprocessors, response synthesizers, and structured outputs.\n",
    "\n",
    "**LEARNING GOALS:**\n",
    "- Understand and implement node postprocessors for filtering and reranking\n",
    "- Learn different response synthesis strategies (TreeSummarize, Refine)\n",
    "- Create structured outputs using Pydantic models\n",
    "- Build advanced retrieval pipelines with multiple processing stages\n",
    "\n",
    "**DATASET:** Use the same data folder as Assignment 1 (`data/`)\n",
    "\n",
    "**PREREQUISITES:** Complete Assignment 1 first\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1. Configure your OpenAI API key when prompted\n",
    "2. Run each cell in order\n",
    "3. Each technique builds on the previous one\n",
    "4. Functions are already implemented - focus on understanding the concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîë Setup: Configure Your OpenAI API Key\n",
    "\n",
    "**REQUIRED for this assignment:** Advanced RAG techniques use LLM operations that require an API key.\n",
    "\n",
    "### Get Your API Key:\n",
    "1. Go to: https://platform.openai.com/api-keys\n",
    "2. Sign up or log in\n",
    "3. Create a new API key\n",
    "4. Copy the key (starts with `sk-proj-...` or `sk-...`)\n",
    "\n",
    "### Cost Estimate:\n",
    "- Model: GPT-4o-mini (~$0.15 per 1M input tokens, ~$0.60 per 1M output tokens)\n",
    "- This assignment: ~10-20 queries √ó ~500 tokens each = **$0.01 - $0.02 total cost**\n",
    "- Very affordable for learning!\n",
    "\n",
    "### How to Enter Your API Key:\n",
    "Run the cell below and paste your API key when prompted. It will be securely stored for this session only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Key Configuration (REQUIRED)\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Check if API key is already set in environment\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"\\nüîë OpenAI API Key Required\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"This assignment uses OpenAI GPT-4o-mini for LLM operations.\")\n",
    "    print(\"\\nGet your API key from: https://platform.openai.com/api-keys\")\n",
    "    print(\"Expected cost: ~$0.01-0.02 for this entire assignment\\n\")\n",
    "    \n",
    "    api_key = getpass(\"Paste your OpenAI API key: \").strip()\n",
    "    \n",
    "    if api_key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "        print(\"\\n‚úÖ OpenAI API key configured successfully!\")\n",
    "        print(\"   You're ready for advanced RAG operations.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No API key entered. LLM operations will fail.\")\n",
    "        print(\"   Please run this cell again and enter your API key.\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key already configured in environment\")\n",
    "    print(\"   Ready for advanced RAG operations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö Step 1: Import Advanced RAG Libraries\n",
    "\n",
    "**What this does:**\n",
    "- Imports all necessary components for advanced RAG techniques\n",
    "- Includes postprocessors, response synthesizers, and output parsers\n",
    "- Imports Pydantic for structured outputs\n",
    "\n",
    "**New Components (vs Assignment 1):**\n",
    "- `SimilarityPostprocessor`: Filters low-quality results\n",
    "- `TreeSummarize`, `Refine`: Different ways to synthesize answers\n",
    "- `PydanticOutputParser`: Creates structured, validated outputs\n",
    "- `OpenAI`: LLM integration for generating responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for advanced RAG\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Core LlamaIndex components\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "# Vector store\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "# Embeddings and LLM (Using OpenAI)\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Advanced RAG components\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.response_synthesizers import TreeSummarize, Refine, CompactAndRefine\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "\n",
    "print(\"‚úÖ Advanced RAG libraries imported successfully!\")\n",
    "print(\"   Using OpenAI for LLM operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öôÔ∏è Step 2: Configure Advanced RAG Settings\n",
    "\n",
    "**What this does:**\n",
    "- Configures OpenAI GPT-4o-mini as the LLM (for generating responses)\n",
    "- Uses local HuggingFace embeddings (same as Assignment 1, free!)\n",
    "- Sets optimized chunk size for better precision\n",
    "\n",
    "**Why GPT-4o-mini?**\n",
    "- ‚úÖ Cost-effective (~10x cheaper than GPT-4)\n",
    "- ‚úÖ Fast responses (~1-2 seconds)\n",
    "- ‚úÖ Good quality for learning and many applications\n",
    "- ‚úÖ Perfect for this assignment (~$0.01-0.02 total)\n",
    "\n",
    "**Temperature = 0.1:**\n",
    "- Low temperature = More consistent, focused responses\n",
    "- Good for factual RAG applications\n",
    "- Less creative randomness\n",
    "\n",
    "**Chunk Size = 512:**\n",
    "- Smaller chunks = Better precision (find exact relevant parts)\n",
    "- Assignment 1 used default (~1024)\n",
    "- 512 is optimized for detailed retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Advanced RAG Settings (Using OpenAI)\n",
    "def setup_advanced_rag_settings():\n",
    "    \"\"\"\n",
    "    Configure LlamaIndex with optimized settings for advanced RAG.\n",
    "    Uses local embeddings and OpenAI for LLM operations.\n",
    "    \"\"\"\n",
    "    # Check for OpenAI API key\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"‚ö†Ô∏è  OPENAI_API_KEY not found!\")\n",
    "        print(\"   Please run the API key configuration cell above.\")\n",
    "        print(\"   LLM operations will fail without an API key.\")\n",
    "        return False\n",
    "    \n",
    "    print(\"‚úÖ OpenAI API key found - configuring advanced RAG...\")\n",
    "    \n",
    "    # Configure OpenAI LLM\n",
    "    Settings.llm = OpenAI(\n",
    "        api_key=api_key,\n",
    "        model=\"gpt-4o-mini\",  # Cost-effective model for learning\n",
    "        temperature=0.1  # Lower temperature for more consistent responses\n",
    "    )\n",
    "    print(\"   Using model: gpt-4o-mini (cost-optimized)\")\n",
    "    print(\"   Temperature: 0.1 (consistent, factual responses)\")\n",
    "    \n",
    "    # Configure local embeddings (no API key required, same as Assignment 1)\n",
    "    print(\"\\nüîÑ Loading local embedding model...\")\n",
    "    Settings.embed_model = HuggingFaceEmbedding(\n",
    "        model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Advanced RAG configuration\n",
    "    Settings.chunk_size = 512  # Smaller chunks for better precision\n",
    "    Settings.chunk_overlap = 50\n",
    "    \n",
    "    print(\"‚úÖ Advanced RAG settings configured successfully!\")\n",
    "    print(\"   - Chunk size: 512 (optimized for precision)\")\n",
    "    print(\"   - Chunk overlap: 50 (maintains context across chunks)\")\n",
    "    print(\"   - Using local embeddings (free, 384 dimensions)\")\n",
    "    print(\"   - OpenAI LLM ready for response synthesis\")\n",
    "    return True\n",
    "\n",
    "# Setup the configuration\n",
    "config_success = setup_advanced_rag_settings()\n",
    "\n",
    "if not config_success:\n",
    "    print(\"\\n‚ùå Configuration failed. Please configure API key above and retry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ Step 3: Create Basic Index (Reuse from Assignment 1)\n",
    "\n",
    "**What this does:**\n",
    "- Creates the foundational vector index that we'll enhance with advanced techniques\n",
    "- Reuses the same concepts from Assignment 1 (document loading, vector store, indexing)\n",
    "- Creates a separate database (`advanced_rag_vectordb`) so it doesn't conflict with Assignment 1\n",
    "\n",
    "**Why a separate database?**\n",
    "- Assignment 1 database: `./assignment_vectordb/`\n",
    "- Assignment 2 database: `./advanced_rag_vectordb/`\n",
    "- Keeps assignments independent\n",
    "- Uses optimized chunk size (512 vs default)\n",
    "\n",
    "**This is the foundation** - Advanced techniques in the following cells will enhance this basic index with:\n",
    "- Similarity filtering\n",
    "- Better response synthesis\n",
    "- Structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Create index from Assignment 1 (reuse the basic functionality)\n",
    "def setup_basic_index(data_folder: str = \"data\", force_rebuild: bool = False):\n",
    "    \"\"\"\n",
    "    Create a basic vector index that we'll enhance with advanced techniques.\n",
    "    This reuses the concepts from Assignment 1.\n",
    "    \"\"\"\n",
    "    # Create vector store\n",
    "    vector_store = LanceDBVectorStore(\n",
    "        uri=\"./advanced_rag_vectordb\",\n",
    "        table_name=\"documents\"\n",
    "    )\n",
    "    \n",
    "    # Load documents\n",
    "    if not Path(data_folder).exists():\n",
    "        print(f\"‚ùå Data folder not found: {data_folder}\")\n",
    "        print(\"   Make sure you're in the correct directory with the 'data' folder.\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"üìÇ Loading documents from: {data_folder}\")\n",
    "    reader = SimpleDirectoryReader(input_dir=data_folder, recursive=True)\n",
    "    documents = reader.load_data()\n",
    "    print(f\"   Loaded {len(documents)} documents\")\n",
    "    \n",
    "    # Create storage context and index\n",
    "    print(\"\\nüîó Creating vector index...\")\n",
    "    print(\"   (This may take 30-60 seconds for ~39 documents...)\")\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents, \n",
    "        storage_context=storage_context,\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Basic index created with {len(documents)} documents\")\n",
    "    print(\"   Ready for advanced RAG techniques!\")\n",
    "    return index\n",
    "\n",
    "# Create the basic index\n",
    "print(\"üöÄ Setting up basic index for advanced RAG...\")\n",
    "print(\"=\" * 50)\n",
    "index = setup_basic_index()\n",
    "\n",
    "if index:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚úÖ Ready to implement advanced RAG techniques!\")\n",
    "    print(\"   The following cells will add:\")\n",
    "    print(\"   1. Similarity filtering (remove irrelevant results)\")\n",
    "    print(\"   2. TreeSummarize (better response synthesis)\")\n",
    "    print(\"   3. Structured outputs (Pydantic models)\")\n",
    "    print(\"   4. Combined advanced pipeline\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to create index - check data folder path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Technique 1: Similarity Filtering (Postprocessor)\n",
    "\n",
    "**What this technique does:**\n",
    "- Filters out retrieved chunks that score below a relevance threshold\n",
    "- Improves response quality by removing \"noise\"\n",
    "- Reduces API costs (fewer tokens sent to LLM)\n",
    "\n",
    "**Key Concept - Postprocessors:**\n",
    "Postprocessors refine retrieval results **after** the initial vector search but **before** sending to the LLM. Think of it as a quality control step.\n",
    "\n",
    "**How Similarity Filtering Works:**\n",
    "1. Vector search retrieves top 10 chunks\n",
    "2. Each chunk has a similarity score (0.0 to 1.0)\n",
    "3. SimilarityPostprocessor filters out chunks below threshold (e.g., 0.3)\n",
    "4. Only high-quality chunks (score ‚â• 0.3) go to the LLM\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query: \"AI agent architectures\"\n",
    "\n",
    "Initial retrieval (10 chunks):\n",
    "- Chunk 1: Score 0.85 ‚úÖ (about AI agents - VERY RELEVANT)\n",
    "- Chunk 2: Score 0.72 ‚úÖ (about agent frameworks - RELEVANT)\n",
    "- Chunk 3: Score 0.65 ‚úÖ (about system design - RELEVANT)\n",
    "- Chunk 4: Score 0.28 ‚ùå (about cooking recipes - NOT RELEVANT)\n",
    "- Chunk 5: Score 0.15 ‚ùå (about finance - NOT RELEVANT)\n",
    "- ... (5 more low-scoring chunks)\n",
    "\n",
    "After SimilarityPostprocessor (cutoff=0.3):\n",
    "- Only Chunks 1, 2, 3 passed (scores ‚â• 0.3)\n",
    "- Result: Cleaner context for LLM, better answers\n",
    "```\n",
    "\n",
    "**Why it matters:**\n",
    "- ‚úÖ Removes irrelevant results that confuse the LLM\n",
    "- ‚úÖ Reduces API costs (fewer tokens)\n",
    "- ‚úÖ Improves answer quality and focus\n",
    "- ‚úÖ Typical cutoff: 0.3 (adjustable based on your needs)\n",
    "\n",
    "**Parameters:**\n",
    "- `similarity_cutoff`: Minimum score (0.0-1.0). Common: 0.3-0.5\n",
    "- `top_k`: How many chunks to retrieve initially (before filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_engine_with_similarity_filter(index, similarity_cutoff: float = 0.3, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Create a query engine that filters results based on similarity scores.\n",
    "    \n",
    "    Args:\n",
    "        index: Vector index to query\n",
    "        similarity_cutoff: Minimum similarity score (0.0 to 1.0)\n",
    "        top_k: Number of initial results to retrieve before filtering\n",
    "        \n",
    "    Returns:\n",
    "        Query engine with similarity filtering\n",
    "    \"\"\"\n",
    "    # Create similarity postprocessor with the cutoff threshold\n",
    "    similarity_processor = SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n",
    "    \n",
    "    # Create query engine with similarity filtering\n",
    "    query_engine = index.as_query_engine(\n",
    "        similarity_top_k=top_k,\n",
    "        node_postprocessors=[similarity_processor]\n",
    "    )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "# Test the function\n",
    "if index:\n",
    "    print(\"üîß Creating query engine with similarity filtering...\")\n",
    "    filtered_engine = create_query_engine_with_similarity_filter(index, similarity_cutoff=0.3)\n",
    "    \n",
    "    if filtered_engine:\n",
    "        print(\"‚úÖ Query engine with similarity filtering created\")\n",
    "        print(\"   Settings: Retrieve 10, filter out scores < 0.3\")\n",
    "        \n",
    "        # Test query\n",
    "        test_query = \"What are the benefits of AI agents?\"\n",
    "        print(f\"\\nüîç Testing query: '{test_query}'\")\n",
    "        print(\"   (This will make an OpenAI API call - ~$0.001 cost)\\n\")\n",
    "        \n",
    "        # Test the response\n",
    "        response = filtered_engine.query(test_query)\n",
    "        print(f\"üìù Filtered Response:\\n{response}\")\n",
    "        \n",
    "        print(\"\\nüí° Notice: Only high-quality, relevant chunks were used!\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to create filtered query engine\")\n",
    "else:\n",
    "    print(\"‚ùå No index available - run previous cells first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üå≥ Technique 2: TreeSummarize (Response Synthesizer)\n",
    "\n",
    "**What this technique does:**\n",
    "- Changes **how** the LLM combines multiple retrieved chunks into a final answer\n",
    "- Uses hierarchical summarization (like building a tree from bottom to top)\n",
    "- Better for complex analytical questions\n",
    "\n",
    "**Key Concept - Response Synthesizers:**\n",
    "Response synthesizers control how retrieved information becomes the final answer. Different strategies work better for different query types.\n",
    "\n",
    "**Available Synthesizers:**\n",
    "1. **TreeSummarize** (this cell):\n",
    "   - Builds response hierarchically\n",
    "   - Summarizes pairs of chunks, then summarizes summaries\n",
    "   - Good for: Comprehensive analysis, \"compare X and Y\", long responses\n",
    "\n",
    "2. **Refine** (not shown here):\n",
    "   - Iteratively improves answer chunk by chunk\n",
    "   - Good for: Detailed explanations, evolving answers\n",
    "\n",
    "3. **CompactAndRefine** (not shown here):\n",
    "   - Combines chunks first, then refines\n",
    "   - Good for: Balance between quality and speed\n",
    "\n",
    "**How TreeSummarize Works:**\n",
    "```\n",
    "Retrieved Chunks: [A, B, C, D]\n",
    "\n",
    "Level 1 (pair summaries):\n",
    "  Summary_AB = Summarize(A, B)\n",
    "  Summary_CD = Summarize(C, D)\n",
    "\n",
    "Level 2 (combine summaries):\n",
    "  Final_Answer = Summarize(Summary_AB, Summary_CD)\n",
    "```\n",
    "\n",
    "**Example Query Types:**\n",
    "- ‚úÖ \"Compare the advantages and disadvantages of X\"\n",
    "- ‚úÖ \"Explain the evolution of Y from early to modern\"\n",
    "- ‚úÖ \"Analyze the relationship between A and B\"\n",
    "- ‚ùå \"What is X?\" (simple factual - default synthesizer is fine)\n",
    "\n",
    "**Why it matters:**\n",
    "- ‚úÖ More comprehensive answers for complex queries\n",
    "- ‚úÖ Better synthesis across multiple sources\n",
    "- ‚úÖ Maintains context across many chunks\n",
    "- ‚ö†Ô∏è Slightly more API calls (but better quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_engine_with_tree_summarize(index, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Create a query engine that uses TreeSummarize for comprehensive responses.\n",
    "    \n",
    "    Args:\n",
    "        index: Vector index to query\n",
    "        top_k: Number of results to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Query engine with TreeSummarize synthesis\n",
    "    \"\"\"\n",
    "    # Create TreeSummarize response synthesizer\n",
    "    tree_synthesizer = TreeSummarize()\n",
    "    \n",
    "    # Create query engine with the synthesizer\n",
    "    query_engine = index.as_query_engine(\n",
    "        similarity_top_k=top_k,\n",
    "        response_synthesizer=tree_synthesizer\n",
    "    )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "# Test the function\n",
    "if index:\n",
    "    print(\"üå≥ Creating query engine with TreeSummarize...\")\n",
    "    tree_engine = create_query_engine_with_tree_summarize(index)\n",
    "    \n",
    "    if tree_engine:\n",
    "        print(\"‚úÖ Query engine with TreeSummarize created\")\n",
    "        print(\"   Best for: Analytical queries, comparisons, comprehensive answers\")\n",
    "        \n",
    "        # Test with a complex analytical query\n",
    "        analytical_query = \"Compare the advantages and disadvantages of different AI agent frameworks\"\n",
    "        print(f\"\\nüîç Testing analytical query: '{analytical_query}'\")\n",
    "        print(\"   (This will make OpenAI API calls for hierarchical summarization)\\n\")\n",
    "        \n",
    "        # Test the response\n",
    "        response = tree_engine.query(analytical_query)\n",
    "        print(f\"üìù TreeSummarize Response:\\n{response}\")\n",
    "        \n",
    "        print(\"\\nüí° Notice: More comprehensive analysis by building answer hierarchically!\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to create TreeSummarize query engine\")\n",
    "else:\n",
    "    print(\"‚ùå No index available - run previous cells first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Technique 3: Structured Outputs (Pydantic Models)\n",
    "\n",
    "**What this technique does:**\n",
    "- Forces LLM to return data in a specific, validated structure\n",
    "- Uses Pydantic models to define the exact output format\n",
    "- Essential for API endpoints, databases, and data pipelines\n",
    "\n",
    "**Key Concept - Structured Outputs:**\n",
    "Instead of free-text responses, you get type-safe, validated data structures that applications can reliably process.\n",
    "\n",
    "**Problem with Free-Text Responses:**\n",
    "```python\n",
    "# Free-text response (unpredictable)\n",
    "response = \"AI agents are systems that can reason. Key capabilities include planning, tool use...\"\n",
    "\n",
    "# How do you extract:\n",
    "# - The title?\n",
    "# - List of key points? (parsing is error-prone)\n",
    "# - Applications? (where do they start/end?)\n",
    "```\n",
    "\n",
    "**Solution with Structured Outputs:**\n",
    "```python\n",
    "# Structured response (predictable)\n",
    "response = ResearchPaperInfo(\n",
    "    title=\"AI Agents and Their Capabilities\",\n",
    "    key_points=[\"reasoning\", \"planning\", \"tool execution\"],\n",
    "    applications=[\"autonomous systems\", \"financial analysis\"],\n",
    "    summary=\"AI agents are autonomous systems...\"\n",
    ")\n",
    "\n",
    "# Easy to use:\n",
    "print(response.title)  # Direct access\n",
    "for point in response.key_points:  # Iterate list\n",
    "    print(point)\n",
    "```\n",
    "\n",
    "**Pydantic Model Example:**\n",
    "```python\n",
    "class ResearchPaperInfo(BaseModel):\n",
    "    title: str  # Must be a string\n",
    "    key_points: List[str]  # Must be a list of strings\n",
    "    applications: List[str]  # Must be a list of strings\n",
    "    summary: str  # Must be a string\n",
    "```\n",
    "\n",
    "**Why it matters:**\n",
    "- ‚úÖ **Predictable outputs** - Always the same structure\n",
    "- ‚úÖ **Type safety** - Pydantic validates data types\n",
    "- ‚úÖ **Easy integration** - Works with databases, APIs, JSON\n",
    "- ‚úÖ **Error prevention** - Catches invalid outputs early\n",
    "\n",
    "**Use Cases:**\n",
    "- REST API endpoints (return JSON)\n",
    "- Database inserts (structured records)\n",
    "- Data pipelines (consistent format)\n",
    "- Frontend applications (predictable data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the Pydantic model for structured outputs  \n",
    "class ResearchPaperInfo(BaseModel):\n",
    "    \"\"\"Structured information about a research paper or AI concept.\"\"\"\n",
    "    title: str = Field(description=\"The main title or concept name\")\n",
    "    key_points: List[str] = Field(description=\"3-5 main points or findings\")\n",
    "    applications: List[str] = Field(description=\"Practical applications or use cases\")\n",
    "    summary: str = Field(description=\"Brief 2-3 sentence summary\")\n",
    "\n",
    "def create_structured_output_program(output_model: BaseModel = ResearchPaperInfo):\n",
    "    \"\"\"\n",
    "    Create a structured output program using Pydantic models.\n",
    "    \n",
    "    Args:\n",
    "        output_model: Pydantic model class for structured output\n",
    "        \n",
    "    Returns:\n",
    "        LLMTextCompletionProgram that returns structured data\n",
    "    \"\"\"\n",
    "    # Create output parser with the Pydantic model\n",
    "    output_parser = PydanticOutputParser(output_cls=output_model)\n",
    "    \n",
    "    # Create the structured output program\n",
    "    prompt_template_str = \"\"\"\n",
    "    Based on the following context and query, extract structured information.\n",
    "    \n",
    "    Context: {context}\n",
    "    Query: {query}\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    \n",
    "    program = LLMTextCompletionProgram.from_defaults(\n",
    "        output_parser=output_parser,\n",
    "        prompt_template_str=prompt_template_str,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    return program\n",
    "\n",
    "# Test the function\n",
    "if index:\n",
    "    print(\"üìä Creating structured output program...\")\n",
    "    structured_program = create_structured_output_program(ResearchPaperInfo)\n",
    "    \n",
    "    if structured_program:\n",
    "        print(\"‚úÖ Structured output program created\")\n",
    "        print(\"   Output format: ResearchPaperInfo (Pydantic model)\")\n",
    "        print(\"   Fields: title, key_points, applications, summary\")\n",
    "        \n",
    "        # Test with retrieval and structured extraction\n",
    "        structure_query = \"Tell me about AI agents and their capabilities\"\n",
    "        print(f\"\\nüîç Testing structured query: '{structure_query}'\")\n",
    "        \n",
    "        # Get context for structured extraction\n",
    "        print(\"   Step 1: Retrieving relevant context...\")\n",
    "        retriever = VectorIndexRetriever(index=index, similarity_top_k=3)\n",
    "        nodes = retriever.retrieve(structure_query)\n",
    "        context = \"\\n\".join([node.text for node in nodes])\n",
    "        print(f\"   Retrieved {len(nodes)} relevant chunks\")\n",
    "        \n",
    "        # Generate structured response\n",
    "        print(\"\\n   Step 2: Generating structured output...\")\n",
    "        print(\"   (This will make an OpenAI API call)\\n\")\n",
    "        response = structured_program(context=context, query=structure_query)\n",
    "        \n",
    "        print(f\"üìä Structured Response:\")\n",
    "        print(f\"\\n   Title: {response.title}\")\n",
    "        print(f\"\\n   Key Points:\")\n",
    "        for i, point in enumerate(response.key_points, 1):\n",
    "            print(f\"      {i}. {point}\")\n",
    "        print(f\"\\n   Applications:\")\n",
    "        for i, app in enumerate(response.applications, 1):\n",
    "            print(f\"      {i}. {app}\")\n",
    "        print(f\"\\n   Summary: {response.summary}\")\n",
    "        \n",
    "        print(\"\\nüí° Output format validated:\")\n",
    "        print(f\"   ‚úÖ Type: {type(response).__name__}\")\n",
    "        print(f\"   ‚úÖ Title: {type(response.title).__name__}\")\n",
    "        print(f\"   ‚úÖ Key points: List with {len(response.key_points)} items\")\n",
    "        print(f\"   ‚úÖ Applications: List with {len(response.applications)} items\")\n",
    "        print(f\"   ‚úÖ Summary: {len(response.summary)} characters\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to create structured output program\")\n",
    "else:\n",
    "    print(\"‚ùå No index available - run previous cells first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Technique 4: Advanced RAG Pipeline (Combining All Techniques)\n",
    "\n",
    "**What this technique does:**\n",
    "- Combines multiple advanced techniques into a single powerful query engine\n",
    "- Similarity filtering **+** TreeSummarize response synthesis\n",
    "- Best of both worlds: Clean results + comprehensive answers\n",
    "\n",
    "**Key Concept - Production RAG Systems:**\n",
    "In real-world applications, you rarely use just one technique. Production RAG systems combine multiple techniques for optimal results.\n",
    "\n",
    "**How the Advanced Pipeline Works:**\n",
    "```\n",
    "User Query: \"Analyze AI agent architectures\"\n",
    "    ‚Üì\n",
    "Step 1: Vector Search\n",
    "    ‚Üí Retrieve top 10 chunks from vector database\n",
    "    ‚Üì\n",
    "Step 2: Similarity Filtering (Postprocessor)\n",
    "    ‚Üí Filter out chunks with score < 0.3\n",
    "    ‚Üí Result: 5-7 high-quality chunks\n",
    "    ‚Üì\n",
    "Step 3: TreeSummarize (Response Synthesizer)\n",
    "    ‚Üí Build hierarchical summary of chunks\n",
    "    ‚Üí Level 1: Pair-wise summaries\n",
    "    ‚Üí Level 2: Combine into final answer\n",
    "    ‚Üì\n",
    "Final Response: Comprehensive, relevant, well-synthesized answer\n",
    "```\n",
    "\n",
    "**Benefits of Combining Techniques:**\n",
    "1. **Similarity Filtering** removes noise ‚Üí Cleaner input for LLM\n",
    "2. **TreeSummarize** builds comprehensive answer ‚Üí Better output quality\n",
    "3. **Together** ‚Üí High-quality results + comprehensive analysis\n",
    "\n",
    "**When to use this:**\n",
    "- ‚úÖ Production applications (where quality matters)\n",
    "- ‚úÖ Complex analytical queries\n",
    "- ‚úÖ When you need both precision and comprehensiveness\n",
    "- ‚úÖ API endpoints serving end users\n",
    "\n",
    "**When NOT to use this:**\n",
    "- ‚ùå Simple factual queries (\"What is X?\") - basic RAG is fine\n",
    "- ‚ùå Extremely cost-sensitive applications - more API calls\n",
    "- ‚ùå Real-time systems needing <100ms response - adds latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_rag_pipeline(index, similarity_cutoff: float = 0.3, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Create a comprehensive advanced RAG pipeline combining multiple techniques.\n",
    "    \n",
    "    Args:\n",
    "        index: Vector index to query\n",
    "        similarity_cutoff: Minimum similarity score for filtering\n",
    "        top_k: Number of initial results to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Advanced query engine with filtering and synthesis combined\n",
    "    \"\"\"\n",
    "    # Create similarity postprocessor\n",
    "    similarity_processor = SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n",
    "    \n",
    "    # Create TreeSummarize for comprehensive responses\n",
    "    tree_synthesizer = TreeSummarize()\n",
    "    \n",
    "    # Create the comprehensive query engine combining both techniques\n",
    "    advanced_engine = index.as_query_engine(\n",
    "        similarity_top_k=top_k,\n",
    "        node_postprocessors=[similarity_processor],\n",
    "        response_synthesizer=tree_synthesizer\n",
    "    )\n",
    "    \n",
    "    return advanced_engine\n",
    "\n",
    "# Test the comprehensive pipeline\n",
    "if index:\n",
    "    print(\"üöÄ Creating advanced RAG pipeline...\")\n",
    "    print(\"   Combining:\")\n",
    "    print(\"   - Similarity filtering (remove noise)\")\n",
    "    print(\"   - TreeSummarize (comprehensive synthesis)\")\n",
    "    \n",
    "    advanced_pipeline = create_advanced_rag_pipeline(index)\n",
    "    \n",
    "    if advanced_pipeline:\n",
    "        print(\"\\n‚úÖ Advanced RAG pipeline created successfully!\")\n",
    "        print(\"   üîß Similarity filtering: ‚úÖ (cutoff 0.3)\")\n",
    "        print(\"   üå≥ TreeSummarize synthesis: ‚úÖ\")\n",
    "        \n",
    "        # Test with complex query\n",
    "        complex_query = \"Analyze the current state and future potential of AI agent technologies\"\n",
    "        print(f\"\\nüîç Testing complex query: '{complex_query}'\")\n",
    "        print(\"   (This combines both techniques for best results)\\n\")\n",
    "        \n",
    "        # Test the response\n",
    "        response = advanced_pipeline.query(complex_query)\n",
    "        print(f\"üöÄ Advanced RAG Response:\\n{response}\")\n",
    "        \n",
    "        print(\"\\nüéØ This response provides:\")\n",
    "        print(\"   ‚úÖ Filtered relevant results only (no noise)\")\n",
    "        print(\"   ‚úÖ Comprehensive analytical response (hierarchical synthesis)\")\n",
    "        print(\"   ‚úÖ Production-quality output\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to create advanced RAG pipeline\")\n",
    "else:\n",
    "    print(\"‚ùå No index available - run previous cells first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üÜö Final Test: Compare Basic vs Advanced RAG\n",
    "\n",
    "**What this cell does:**\n",
    "- Tests the same queries with **basic RAG** vs **advanced RAG**\n",
    "- Shows you the quality improvements from advanced techniques\n",
    "- Validates that all 5 components work correctly\n",
    "\n",
    "**Components to Test:**\n",
    "1. ‚úÖ Basic Index (foundation)\n",
    "2. ‚úÖ Similarity Filter (postprocessor)\n",
    "3. ‚úÖ TreeSummarize (response synthesizer)\n",
    "4. ‚úÖ Structured Output (Pydantic models)\n",
    "5. ‚úÖ Advanced Pipeline (combined techniques)\n",
    "\n",
    "**Test Queries:**\n",
    "- Query 1: Key capabilities (factual)\n",
    "- Query 2: Evaluation metrics (analytical)\n",
    "- Query 3: Benefits and challenges (comparative)\n",
    "\n",
    "**What to look for:**\n",
    "- Basic RAG: Functional answers\n",
    "- Advanced RAG: More focused, comprehensive, better-synthesized answers\n",
    "\n",
    "**Expected differences:**\n",
    "- Advanced responses should be more relevant (filtered)\n",
    "- Advanced responses should be more comprehensive (TreeSummarize)\n",
    "- Less irrelevant information in advanced responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison: Basic vs Advanced RAG\n",
    "print(\"üöÄ Advanced RAG Techniques Assignment - Final Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test queries for comparison\n",
    "test_queries = [\n",
    "    \"What are the key capabilities of AI agents?\",\n",
    "    \"How do you evaluate agent performance metrics?\",\n",
    "    \"Explain the benefits and challenges of multimodal AI systems\"\n",
    "]\n",
    "\n",
    "# Check if all components were created\n",
    "components_status = {\n",
    "    \"Basic Index\": index is not None,\n",
    "    \"Similarity Filter\": 'filtered_engine' in locals() and filtered_engine is not None,\n",
    "    \"TreeSummarize\": 'tree_engine' in locals() and tree_engine is not None,\n",
    "    \"Structured Output\": 'structured_program' in locals() and structured_program is not None,\n",
    "    \"Advanced Pipeline\": 'advanced_pipeline' in locals() and advanced_pipeline is not None\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Component Status:\")\n",
    "for component, status in components_status.items():\n",
    "    status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"   {status_icon} {component}\")\n",
    "\n",
    "# Create basic query engine for comparison\n",
    "if index:\n",
    "    print(\"\\nüîç Creating basic query engine for comparison...\")\n",
    "    basic_engine = index.as_query_engine(similarity_top_k=5)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üÜö COMPARISON: Basic vs Advanced RAG\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚è±Ô∏è  Note: This will make multiple OpenAI API calls (~$0.03-0.05 total)\")\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüìã Test Query {i}: '{query}'\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Basic RAG\n",
    "        print(\"üîπ Basic RAG:\")\n",
    "        if basic_engine:\n",
    "            basic_response = basic_engine.query(query)\n",
    "            print(f\"   {str(basic_response)[:200]}...\")\n",
    "        \n",
    "        # Advanced RAG (if implemented)\n",
    "        print(\"\\nüî∏ Advanced RAG:\")\n",
    "        if components_status[\"Advanced Pipeline\"]:\n",
    "            advanced_response = advanced_pipeline.query(query)\n",
    "            print(f\"   {str(advanced_response)[:200]}...\")\n",
    "        else:\n",
    "            print(\"   Complete the advanced pipeline function to test\")\n",
    "\n",
    "# Final status\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ Assignment Status:\")\n",
    "completed_count = sum(components_status.values())\n",
    "total_count = len(components_status)\n",
    "\n",
    "print(f\"   Completed: {completed_count}/{total_count} components\")\n",
    "\n",
    "if completed_count == total_count:\n",
    "    print(\"\\nüéâ Congratulations! You've mastered Advanced RAG Techniques!\")\n",
    "    print(\"   ‚úÖ Node postprocessors for result filtering\")\n",
    "    print(\"   ‚úÖ Response synthesizers for better answers\")\n",
    "    print(\"   ‚úÖ Structured outputs for reliable data\")\n",
    "    print(\"   ‚úÖ Advanced pipelines combining all techniques\")\n",
    "    print(\"\\nüöÄ You're ready for production RAG systems!\")\n",
    "    print(\"\\nüìö Key Takeaways:\")\n",
    "    print(\"   ‚Ä¢ Postprocessors filter noise ‚Üí Better input quality\")\n",
    "    print(\"   ‚Ä¢ TreeSummarize builds comprehensive answers ‚Üí Better output quality\")\n",
    "    print(\"   ‚Ä¢ Structured outputs enable system integration ‚Üí Production-ready\")\n",
    "    print(\"   ‚Ä¢ Combining techniques ‚Üí Professional RAG applications\")\n",
    "else:\n",
    "    missing = total_count - completed_count\n",
    "    print(f\"\\nüìù {missing} component(s) need attention:\")\n",
    "    for component, status in components_status.items():\n",
    "        if not status:\n",
    "            print(f\"   ‚ùå {component}\")\n",
    "\n",
    "print(\"\\nüí° Advanced RAG vs Basic RAG:\")\n",
    "print(\"   Basic: Good for simple queries, fast responses\")\n",
    "print(\"   Advanced: Better quality, comprehensive answers, production-ready\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
