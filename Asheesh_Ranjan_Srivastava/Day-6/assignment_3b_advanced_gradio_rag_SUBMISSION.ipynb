{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3b: Advanced Gradio RAG Frontend\n",
    "## Day 6 Session 2 - Building Configurable RAG Applications\n",
    "\n",
    "In this assignment, you'll extend your basic RAG interface with advanced configuration options to create a professional, feature-rich RAG application.\n",
    "\n",
    "**New Features to Add:**\n",
    "- Model selection dropdown (gpt-4o, gpt-4o-mini)\n",
    "- Temperature slider (0 to 1 with 0.1 intervals)\n",
    "- Chunk size configuration\n",
    "- Chunk overlap configuration  \n",
    "- Similarity top-k slider\n",
    "- Node postprocessor multiselect\n",
    "- Similarity cutoff slider\n",
    "- Response synthesizer multiselect\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Advanced Gradio components and interactions\n",
    "- Dynamic RAG configuration\n",
    "- Professional UI design patterns\n",
    "- Parameter validation and handling\n",
    "- Building production-ready AI applications\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed Assignment 3a (Basic Gradio RAG)\n",
    "- Understanding of RAG parameters and their effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîë Setup: Configure Your API Key\n",
    "\n",
    "**This assignment uses OpenRouter** (cheaper alternative to OpenAI direct).\n",
    "\n",
    "### Get Your OpenRouter API Key:\n",
    "1. Go to: https://openrouter.ai/keys\n",
    "2. Sign up or log in (supports Google sign-in)\n",
    "3. Create a new API key\n",
    "4. Copy the key (starts with `sk-or-v1-...`)\n",
    "\n",
    "### Why OpenRouter?\n",
    "- ‚úÖ Access to multiple models (GPT-4, Claude, Gemini, etc.)\n",
    "- ‚úÖ Often cheaper than direct OpenAI access\n",
    "- ‚úÖ Easy to compare models\n",
    "- ‚úÖ Good for learning and experimentation\n",
    "\n",
    "### Cost Estimate:\n",
    "- Using GPT-4o-mini via OpenRouter\n",
    "- This assignment: ~10-15 queries with different configs = **$0.01 - $0.02 total**\n",
    "- Very affordable!\n",
    "\n",
    "**Alternative:** You can also use OpenAI API key directly if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key Configuration\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Check if API key is already set\n",
    "if not os.getenv(\"OPENROUTER_API_KEY\") and not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"\\nüîë API Key Configuration\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"This assignment needs an LLM API key.\\n\")\n",
    "    print(\"Option 1 (Recommended): OpenRouter API key\")\n",
    "    print(\"  Get from: https://openrouter.ai/keys\")\n",
    "    print(\"  Format: sk-or-v1-...\")\n",
    "    print(\"  Benefit: Access to multiple models, often cheaper\\n\")\n",
    "    print(\"Option 2: OpenAI API key\")\n",
    "    print(\"  Get from: https://platform.openai.com/api-keys\")\n",
    "    print(\"  Format: sk-proj-... or sk-...\\n\")\n",
    "    \n",
    "    api_key = getpass(\"Paste your API key: \").strip()\n",
    "    \n",
    "    if api_key:\n",
    "        if api_key.startswith(\"sk-or-\"):\n",
    "            os.environ[\"OPENROUTER_API_KEY\"] = api_key\n",
    "            print(\"\\n‚úÖ OpenRouter API key configured!\")\n",
    "        elif api_key.startswith(\"sk-\"):\n",
    "            os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "            print(\"\\n‚úÖ OpenAI API key configured!\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Warning: API key format not recognized. Setting as OpenRouter key.\")\n",
    "            os.environ[\"OPENROUTER_API_KEY\"] = api_key\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No API key entered. Please run this cell again.\")\n",
    "else:\n",
    "    if os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "        print(\"‚úÖ OpenRouter API key already configured!\")\n",
    "    else:\n",
    "        print(\"‚úÖ OpenAI API key already configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö Part 1: Setup and Imports\n",
    "\n",
    "**What's new vs Assignment 3a:**\n",
    "- Advanced RAG components (postprocessors, synthesizers)\n",
    "- More sophisticated configuration handling\n",
    "\n",
    "**Libraries:**\n",
    "- **Gradio**: Web UI framework\n",
    "- **LlamaIndex Core**: Basic RAG components\n",
    "- **LlamaIndex Advanced**: Postprocessors and response synthesizers\n",
    "- **OpenRouter**: LLM access (multi-model support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import gradio as gr\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "# LlamaIndex core components\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openrouter import OpenRouter\n",
    "\n",
    "# Advanced RAG components\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.response_synthesizers import TreeSummarize, Refine, CompactAndRefine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ Part 2: Advanced RAG Backend Class\n",
    "\n",
    "**What this class does:**\n",
    "- Supports dynamic configuration of ALL RAG parameters\n",
    "- Handles multiple postprocessors and synthesizers\n",
    "- Returns detailed results including sources and config used\n",
    "\n",
    "**Key Methods:**\n",
    "1. `update_settings()` - Dynamically update LLM, temperature, chunking\n",
    "2. `initialize_database()` - Load documents and create vector index\n",
    "3. `get_postprocessor()` - Create configured postprocessor\n",
    "4. `get_synthesizer()` - Create configured response synthesizer\n",
    "5. `advanced_query()` - Process queries with full configuration\n",
    "\n",
    "**Configuration Options:**\n",
    "- **Model**: Which LLM to use (gpt-4o, gpt-4o-mini, etc.)\n",
    "- **Temperature**: Randomness of responses (0.0-1.0)\n",
    "- **Chunk Size**: How much text per chunk (256-1024)\n",
    "- **Chunk Overlap**: Context preserved between chunks (10-100)\n",
    "- **Similarity Top-K**: How many chunks to retrieve (1-20)\n",
    "- **Postprocessors**: Filters for retrieved chunks\n",
    "- **Similarity Cutoff**: Minimum score for postprocessor (0.0-1.0)\n",
    "- **Response Synthesizer**: How to combine chunks into answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAGBackend:\n",
    "    \"\"\"Advanced RAG backend with configurable parameters.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.index = None\n",
    "        self.available_models = [\"openai/gpt-4o\", \"openai/gpt-4o-mini\"]\n",
    "        self.available_postprocessors = [\"SimilarityPostprocessor\", \"None\"]\n",
    "        self.available_synthesizers = [\"TreeSummarize\", \"Refine\", \"CompactAndRefine\", \"Default\"]\n",
    "        self.update_settings()\n",
    "        \n",
    "    def update_settings(self, model: str = \"openai/gpt-4o-mini\", temperature: float = 0.1, \n",
    "                       chunk_size: int = 512, chunk_overlap: int = 50):\n",
    "        \"\"\"Update LlamaIndex settings based on user configuration.\"\"\"\n",
    "        # Try OpenRouter first, fall back to OpenAI\n",
    "        openrouter_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "        openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        \n",
    "        if openrouter_key:\n",
    "            Settings.llm = OpenRouter(\n",
    "                api_key=openrouter_key,\n",
    "                model=model,\n",
    "                temperature=temperature\n",
    "            )\n",
    "        elif openai_key:\n",
    "            from llama_index.llms.openai import OpenAI\n",
    "            # Extract model name (remove \"openai/\" prefix if present)\n",
    "            model_name = model.replace(\"openai/\", \"\")\n",
    "            Settings.llm = OpenAI(\n",
    "                api_key=openai_key,\n",
    "                model=model_name,\n",
    "                temperature=temperature\n",
    "            )\n",
    "        \n",
    "        # Set up the embedding model (keep this constant - local and free)\n",
    "        Settings.embed_model = HuggingFaceEmbedding(\n",
    "            model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Set chunking parameters from function parameters\n",
    "        Settings.chunk_size = chunk_size\n",
    "        Settings.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def initialize_database(self, data_folder=\"data\"):\n",
    "        \"\"\"Initialize the vector database with documents.\"\"\"\n",
    "        if not Path(data_folder).exists():\n",
    "            return f\"‚ùå Data folder '{data_folder}' not found! Please check the path.\"\n",
    "        \n",
    "        try:\n",
    "            vector_store = LanceDBVectorStore(\n",
    "                uri=\"./advanced_rag_vectordb\",\n",
    "                table_name=\"documents\"\n",
    "            )\n",
    "            \n",
    "            reader = SimpleDirectoryReader(input_dir=data_folder, recursive=True)\n",
    "            documents = reader.load_data()\n",
    "            \n",
    "            if len(documents) == 0:\n",
    "                return f\"‚ùå No documents found in '{data_folder}'!\"\n",
    "            \n",
    "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "            self.index = VectorStoreIndex.from_documents(\n",
    "                documents, \n",
    "                storage_context=storage_context,\n",
    "                show_progress=True\n",
    "            )\n",
    "            \n",
    "            return f\"‚úÖ Database initialized successfully with {len(documents)} documents!\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error initializing database: {str(e)}\"\n",
    "    \n",
    "    def get_postprocessor(self, postprocessor_name: str, similarity_cutoff: float):\n",
    "        \"\"\"Get the selected postprocessor.\"\"\"\n",
    "        if postprocessor_name == \"SimilarityPostprocessor\":\n",
    "            return SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n",
    "        return None\n",
    "    \n",
    "    def get_synthesizer(self, synthesizer_name: str):\n",
    "        \"\"\"Get the selected response synthesizer.\"\"\"\n",
    "        if synthesizer_name == \"TreeSummarize\":\n",
    "            return TreeSummarize()\n",
    "        elif synthesizer_name == \"Refine\":\n",
    "            return Refine()\n",
    "        elif synthesizer_name == \"CompactAndRefine\":\n",
    "            return CompactAndRefine()\n",
    "        return None  # Default synthesizer\n",
    "    \n",
    "    def advanced_query(self, question: str, model: str, temperature: float, \n",
    "                      chunk_size: int, chunk_overlap: int, similarity_top_k: int,\n",
    "                      postprocessor_names: List[str], similarity_cutoff: float,\n",
    "                      synthesizer_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Query the RAG system with advanced configuration.\"\"\"\n",
    "        \n",
    "        if self.index is None:\n",
    "            return {\"response\": \"‚ùå Please initialize the database first!\", \"sources\": [], \"config\": {}}\n",
    "        \n",
    "        if not question or not question.strip():\n",
    "            return {\"response\": \"‚ö†Ô∏è Please enter a question first!\", \"sources\": [], \"config\": {}}\n",
    "        \n",
    "        try:\n",
    "            # Update settings with new parameters\n",
    "            self.update_settings(model, temperature, chunk_size, chunk_overlap)\n",
    "            \n",
    "            # Get postprocessors\n",
    "            postprocessors = []\n",
    "            for name in postprocessor_names:\n",
    "                processor = self.get_postprocessor(name, similarity_cutoff)\n",
    "                if processor is not None:\n",
    "                    postprocessors.append(processor)\n",
    "            \n",
    "            # Get synthesizer\n",
    "            synthesizer = self.get_synthesizer(synthesizer_name)\n",
    "            \n",
    "            # Create query engine with all parameters\n",
    "            query_engine_kwargs = {\"similarity_top_k\": similarity_top_k}\n",
    "            if postprocessors:\n",
    "                query_engine_kwargs[\"node_postprocessors\"] = postprocessors\n",
    "            if synthesizer is not None:\n",
    "                query_engine_kwargs[\"response_synthesizer\"] = synthesizer\n",
    "            \n",
    "            query_engine = self.index.as_query_engine(**query_engine_kwargs)\n",
    "            \n",
    "            # Query and get response\n",
    "            response = query_engine.query(question)\n",
    "            \n",
    "            # Extract source information if available\n",
    "            sources = []\n",
    "            if hasattr(response, 'source_nodes'):\n",
    "                for node in response.source_nodes:\n",
    "                    sources.append({\n",
    "                        \"text\": node.text[:200] + \"...\",\n",
    "                        \"score\": getattr(node, 'score', 0.0),\n",
    "                        \"source\": getattr(node.node, 'metadata', {}).get('file_name', 'Unknown')\n",
    "                    })\n",
    "            \n",
    "            return {\n",
    "                \"response\": str(response),\n",
    "                \"sources\": sources,\n",
    "                \"config\": {\n",
    "                    \"model\": model,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"chunk_size\": chunk_size,\n",
    "                    \"chunk_overlap\": chunk_overlap,\n",
    "                    \"similarity_top_k\": similarity_top_k,\n",
    "                    \"postprocessors\": postprocessor_names,\n",
    "                    \"similarity_cutoff\": similarity_cutoff,\n",
    "                    \"synthesizer\": synthesizer_name\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\"response\": f\"‚ùå Error processing query: {str(e)}\", \"sources\": [], \"config\": {}}\n",
    "\n",
    "# Initialize the backend\n",
    "print(\"üöÄ Initializing Advanced RAG Backend...\")\n",
    "rag_backend = AdvancedRAGBackend()\n",
    "print(\"‚úÖ Advanced RAG Backend initialized and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üé® Part 3: Advanced Gradio Interface\n",
    "\n",
    "**What you'll build:**\n",
    "A sophisticated 2-column layout:\n",
    "- **Left Column**: All configuration controls\n",
    "- **Right Column**: Query interface and responses\n",
    "\n",
    "**Components Needed:**\n",
    "\n",
    "### Configuration Controls (Left):\n",
    "1. **Model Dropdown** - `gr.Dropdown(choices=[...], value=\"...\")`\n",
    "2. **Temperature Slider** - `gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.1)`\n",
    "3. **Chunk Size Number** - `gr.Number(value=512, minimum=128, maximum=2048)`\n",
    "4. **Chunk Overlap Number** - `gr.Number(value=50, minimum=0, maximum=200)`\n",
    "5. **Similarity Top-K Slider** - `gr.Slider(minimum=1, maximum=20, step=1, value=5)`\n",
    "6. **Postprocessor Checkbox** - `gr.CheckboxGroup(choices=[...], value=[...])`\n",
    "7. **Similarity Cutoff Slider** - `gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.3)`\n",
    "8. **Synthesizer Dropdown** - `gr.Dropdown(choices=[...], value=\"Default\")`\n",
    "\n",
    "### Query Interface (Right):\n",
    "1. **Query Input** - `gr.Textbox(lines=3, placeholder=\"...\")`\n",
    "2. **Submit Button** - `gr.Button(variant=\"primary\")`\n",
    "3. **Response Output** - `gr.Textbox(lines=12, interactive=False)`\n",
    "4. **Config Display** - `gr.Textbox(lines=8, interactive=False)`\n",
    "\n",
    "**Layout Pattern:**\n",
    "```python\n",
    "with gr.Blocks() as interface:\n",
    "    # Title\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):  # Left - Config\n",
    "            # Configuration controls\n",
    "        with gr.Column(scale=2):  # Right - Query\n",
    "            # Query interface\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_rag_interface():\n",
    "    \"\"\"Create advanced RAG interface with full configuration options.\"\"\"\n",
    "    \n",
    "    def initialize_db():\n",
    "        \"\"\"Handle database initialization.\"\"\"\n",
    "        return rag_backend.initialize_database()\n",
    "    \n",
    "    def handle_advanced_query(question, model, temperature, chunk_size, chunk_overlap, \n",
    "                             similarity_top_k, postprocessors, similarity_cutoff, synthesizer):\n",
    "        \"\"\"Handle advanced RAG queries with all configuration options.\"\"\"\n",
    "        result = rag_backend.advanced_query(\n",
    "            question, model, temperature, chunk_size, chunk_overlap,\n",
    "            similarity_top_k, postprocessors, similarity_cutoff, synthesizer\n",
    "        )\n",
    "        \n",
    "        # Format configuration for display\n",
    "        config_text = f\"\"\"**Current Configuration:**\n",
    "- Model: {result['config'].get('model', 'N/A')}\n",
    "- Temperature: {result['config'].get('temperature', 'N/A')}\n",
    "- Chunk Size: {result['config'].get('chunk_size', 'N/A')}\n",
    "- Chunk Overlap: {result['config'].get('chunk_overlap', 'N/A')}\n",
    "- Similarity Top-K: {result['config'].get('similarity_top_k', 'N/A')}\n",
    "- Postprocessors: {', '.join(result['config'].get('postprocessors', []))}\n",
    "- Similarity Cutoff: {result['config'].get('similarity_cutoff', 'N/A')}\n",
    "- Synthesizer: {result['config'].get('synthesizer', 'N/A')}\"\"\"\n",
    "        \n",
    "        return result[\"response\"], config_text\n",
    "    \n",
    "    # Create the advanced interface structure\n",
    "    with gr.Blocks(title=\"Advanced RAG Assistant\") as interface:\n",
    "        # Title and description\n",
    "        gr.Markdown(\"# ü§ñ Advanced RAG Assistant\")\n",
    "        gr.Markdown(\"Configure all RAG parameters and experiment with different settings!\")\n",
    "        gr.Markdown(\"---\")\n",
    "        \n",
    "        # Database initialization section\n",
    "        gr.Markdown(\"### üöÄ Step 1: Initialize Database\")\n",
    "        init_btn = gr.Button(\"Initialize Vector Database\", variant=\"primary\", size=\"lg\")\n",
    "        status_output = gr.Textbox(\n",
    "            label=\"Status\",\n",
    "            placeholder=\"Click 'Initialize Vector Database' to start...\",\n",
    "            interactive=False,\n",
    "            lines=2\n",
    "        )\n",
    "        \n",
    "        gr.Markdown(\"---\")\n",
    "        gr.Markdown(\"### üí¨ Step 2: Configure & Query\")\n",
    "        \n",
    "        # Main layout with columns\n",
    "        with gr.Row():\n",
    "            # Left column: Configuration controls\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"#### ‚öôÔ∏è RAG Configuration\")\n",
    "                \n",
    "                # Model selection\n",
    "                model_dropdown = gr.Dropdown(\n",
    "                    choices=[\"openai/gpt-4o\", \"openai/gpt-4o-mini\"],\n",
    "                    value=\"openai/gpt-4o-mini\",\n",
    "                    label=\"Model\",\n",
    "                    info=\"Choose LLM model (gpt-4o-mini is faster & cheaper)\"\n",
    "                )\n",
    "                \n",
    "                # Temperature control\n",
    "                temperature_slider = gr.Slider(\n",
    "                    minimum=0.0,\n",
    "                    maximum=1.0,\n",
    "                    step=0.1,\n",
    "                    value=0.1,\n",
    "                    label=\"Temperature\",\n",
    "                    info=\"0.0 = deterministic, 1.0 = creative\"\n",
    "                )\n",
    "                \n",
    "                gr.Markdown(\"**Chunking Parameters:**\")\n",
    "                \n",
    "                # Chunk size\n",
    "                chunk_size_input = gr.Number(\n",
    "                    value=512,\n",
    "                    minimum=128,\n",
    "                    maximum=2048,\n",
    "                    label=\"Chunk Size\",\n",
    "                    info=\"Characters per chunk (default: 512)\"\n",
    "                )\n",
    "                \n",
    "                # Chunk overlap\n",
    "                chunk_overlap_input = gr.Number(\n",
    "                    value=50,\n",
    "                    minimum=0,\n",
    "                    maximum=200,\n",
    "                    label=\"Chunk Overlap\",\n",
    "                    info=\"Overlap between chunks (default: 50)\"\n",
    "                )\n",
    "                \n",
    "                gr.Markdown(\"**Retrieval Parameters:**\")\n",
    "                \n",
    "                # Similarity top-k\n",
    "                similarity_topk_slider = gr.Slider(\n",
    "                    minimum=1,\n",
    "                    maximum=20,\n",
    "                    step=1,\n",
    "                    value=5,\n",
    "                    label=\"Similarity Top-K\",\n",
    "                    info=\"Number of chunks to retrieve\"\n",
    "                )\n",
    "                \n",
    "                # Postprocessor selection\n",
    "                postprocessor_checkbox = gr.CheckboxGroup(\n",
    "                    choices=[\"SimilarityPostprocessor\", \"None\"],\n",
    "                    value=[\"SimilarityPostprocessor\"],\n",
    "                    label=\"Node Postprocessors\",\n",
    "                    info=\"Filters for retrieved chunks\"\n",
    "                )\n",
    "                \n",
    "                # Similarity cutoff\n",
    "                similarity_cutoff_slider = gr.Slider(\n",
    "                    minimum=0.0,\n",
    "                    maximum=1.0,\n",
    "                    step=0.1,\n",
    "                    value=0.3,\n",
    "                    label=\"Similarity Cutoff\",\n",
    "                    info=\"Minimum relevance score (0.3 recommended)\"\n",
    "                )\n",
    "                \n",
    "                # Response synthesizer\n",
    "                synthesizer_dropdown = gr.Dropdown(\n",
    "                    choices=[\"Default\", \"TreeSummarize\", \"Refine\", \"CompactAndRefine\"],\n",
    "                    value=\"Default\",\n",
    "                    label=\"Response Synthesizer\",\n",
    "                    info=\"How to combine retrieved chunks\"\n",
    "                )\n",
    "            \n",
    "            # Right column: Query interface\n",
    "            with gr.Column(scale=2):\n",
    "                gr.Markdown(\"#### üí¨ Query Interface\")\n",
    "                \n",
    "                # Query input\n",
    "                query_input = gr.Textbox(\n",
    "                    label=\"Your Question\",\n",
    "                    placeholder=\"What would you like to know about the documents?\",\n",
    "                    lines=3\n",
    "                )\n",
    "                \n",
    "                # Submit button\n",
    "                submit_btn = gr.Button(\"üîç Ask Question\", variant=\"primary\", size=\"lg\")\n",
    "                \n",
    "                # Response output\n",
    "                response_output = gr.Textbox(\n",
    "                    label=\"AI Response\",\n",
    "                    placeholder=\"Response will appear here...\",\n",
    "                    interactive=False,\n",
    "                    lines=12\n",
    "                )\n",
    "                \n",
    "                # Configuration display\n",
    "                config_display = gr.Textbox(\n",
    "                    label=\"Configuration Used\",\n",
    "                    placeholder=\"Configuration details will appear here...\",\n",
    "                    interactive=False,\n",
    "                    lines=8\n",
    "                )\n",
    "        \n",
    "        # Connect functions to components\n",
    "        init_btn.click(initialize_db, outputs=[status_output])\n",
    "        \n",
    "        submit_btn.click(\n",
    "            handle_advanced_query,\n",
    "            inputs=[\n",
    "                query_input, model_dropdown, temperature_slider,\n",
    "                chunk_size_input, chunk_overlap_input, similarity_topk_slider,\n",
    "                postprocessor_checkbox, similarity_cutoff_slider, synthesizer_dropdown\n",
    "            ],\n",
    "            outputs=[response_output, config_display]\n",
    "        )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Create the interface\n",
    "print(\"üé® Creating advanced Gradio interface...\")\n",
    "advanced_interface = create_advanced_rag_interface()\n",
    "print(\"‚úÖ Advanced RAG interface created successfully!\")\n",
    "print(\"\\nüí° Run the next cell to launch the app!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Part 4: Launch Your Advanced Application\n",
    "\n",
    "**What this does:**\n",
    "- Starts a local web server with your advanced RAG interface\n",
    "- Opens in browser at http://localhost:7860\n",
    "- Provides full configurability of RAG parameters\n",
    "\n",
    "**Testing Strategy:**\n",
    "\n",
    "### 1. Baseline Test (Default Settings):\n",
    "- Initialize database\n",
    "- Ask: \"What are AI agents?\"\n",
    "- Note the response quality and configuration\n",
    "\n",
    "### 2. Model Comparison:\n",
    "- **Test 1**: gpt-4o-mini, temperature 0.1\n",
    "- **Test 2**: gpt-4o, temperature 0.1 (same question)\n",
    "- **Compare**: Quality difference vs cost\n",
    "\n",
    "### 3. Temperature Experiment:\n",
    "- **Test 1**: Temperature 0.1 (deterministic)\n",
    "- **Test 2**: Temperature 0.9 (creative)\n",
    "- **Compare**: Consistency vs creativity\n",
    "\n",
    "### 4. Chunk Size Impact:\n",
    "- **Test 1**: Chunk size 256 (fine-grained)\n",
    "- **Test 2**: Chunk size 1024 (coarse-grained)\n",
    "- **Compare**: Precision vs context\n",
    "\n",
    "### 5. Synthesizer Comparison:\n",
    "- **Test 1**: Default synthesizer\n",
    "- **Test 2**: TreeSummarize\n",
    "- **Test 3**: Refine\n",
    "- **Compare**: Response structure and quality\n",
    "\n",
    "### 6. Filtering Effects:\n",
    "- **Test 1**: Similarity cutoff 0.1 (permissive)\n",
    "- **Test 2**: Similarity cutoff 0.7 (strict)\n",
    "- **Compare**: Relevance vs completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Launching your Advanced RAG Assistant...\")\n",
    "print(\"üîó Your application will open in a new browser tab!\")\n",
    "print(\"\")\n",
    "print(\"‚ö†Ô∏è  Important: Make sure your API key is configured (run first cell if needed)\")\n",
    "print(\"\")\n",
    "print(\"üìã Testing Instructions:\")\n",
    "print(\"1. Click 'Initialize Vector Database' button first\")\n",
    "print(\"2. Wait for success message (~30-60 seconds)\")\n",
    "print(\"3. Configure your RAG parameters in the left column:\")\n",
    "print(\"   - Choose model (gpt-4o, gpt-4o-mini)\")\n",
    "print(\"   - Adjust temperature (0.0 = deterministic, 1.0 = creative)\")\n",
    "print(\"   - Set chunk size and overlap\")\n",
    "print(\"   - Choose similarity top-k\")\n",
    "print(\"   - Select postprocessors and synthesizer\")\n",
    "print(\"4. Enter a question in the right column\")\n",
    "print(\"5. Click 'Ask Question'\")\n",
    "print(\"6. Review both the response and configuration used\")\n",
    "print(\"\")\n",
    "print(\"üß™ Experiments to try:\")\n",
    "print(\"- Compare gpt-4o vs gpt-4o-mini with same question\")\n",
    "print(\"- Test temperature effects (0.1 vs 0.9)\")\n",
    "print(\"- Try different chunk sizes (256 vs 1024)\")\n",
    "print(\"- Compare synthesizers (Default vs TreeSummarize vs Refine)\")\n",
    "print(\"- Adjust similarity cutoff (0.1 vs 0.7) to see filtering\")\n",
    "print(\"\")\n",
    "print(\"üí° Example questions:\")\n",
    "print(\"- What are the main topics covered in the documents?\")\n",
    "print(\"- Compare and contrast different AI agent architectures\")\n",
    "print(\"- How do evaluation metrics work for AI agents?\")\n",
    "print(\"\")\n",
    "print(\"üöÄ Launching app...\")\n",
    "print(\"\")\n",
    "\n",
    "# Launch the application\n",
    "advanced_interface.launch(\n",
    "    server_port=7861,  # Different port from 3a to avoid conflicts\n",
    "    share=False,       # Set to True for public URL (72 hours)\n",
    "    inline=False       # Set to True to display inline in Jupyter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üí° Understanding the Configuration Options\n",
    "\n",
    "### Model Selection\n",
    "**What it controls**: Which LLM processes your query and generates the response.\n",
    "\n",
    "- **gpt-4o**: Latest and most capable\n",
    "  - ‚úÖ Best quality responses\n",
    "  - ‚úÖ Better reasoning\n",
    "  - ‚ùå More expensive (~$2.50/$10 per 1M tokens)\n",
    "  - ‚ùå Slower responses\n",
    "\n",
    "- **gpt-4o-mini**: Optimized and efficient\n",
    "  - ‚úÖ Fast responses\n",
    "  - ‚úÖ Very cheap (~$0.15/$0.60 per 1M tokens)\n",
    "  - ‚úÖ Good quality for most tasks\n",
    "  - ‚ùå Slightly less capable for complex reasoning\n",
    "\n",
    "**Recommendation**: Start with gpt-4o-mini, upgrade to gpt-4o if quality insufficient.\n",
    "\n",
    "---\n",
    "\n",
    "### Temperature (0.0 - 1.0)\n",
    "**What it controls**: Randomness/creativity in responses.\n",
    "\n",
    "- **0.0-0.2**: Deterministic, factual\n",
    "  - ‚úÖ Consistent responses\n",
    "  - ‚úÖ Best for facts and data\n",
    "  - ‚ùå Can be repetitive\n",
    "\n",
    "- **0.3-0.7**: Balanced\n",
    "  - ‚úÖ Some variation\n",
    "  - ‚úÖ Still reliable\n",
    "  - Good default\n",
    "\n",
    "- **0.8-1.0**: Creative\n",
    "  - ‚úÖ More varied responses\n",
    "  - ‚úÖ Good for brainstorming\n",
    "  - ‚ùå Less predictable\n",
    "  - ‚ùå May hallucinate\n",
    "\n",
    "**Recommendation**: 0.1 for factual queries, 0.5-0.7 for creative tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Chunk Size & Overlap\n",
    "**What they control**: How documents are split for processing.\n",
    "\n",
    "**Chunk Size** (typical: 256-1024):\n",
    "- **Smaller (256-512)**:\n",
    "  - ‚úÖ More precise retrieval\n",
    "  - ‚úÖ Better for finding specific info\n",
    "  - ‚ùå May miss broader context\n",
    "\n",
    "- **Larger (768-1024)**:\n",
    "  - ‚úÖ More context per chunk\n",
    "  - ‚úÖ Better for understanding relationships\n",
    "  - ‚ùå Less precise\n",
    "  - ‚ùå More tokens (higher cost)\n",
    "\n",
    "**Chunk Overlap** (typical: 10-100):\n",
    "- **Purpose**: Prevents splitting sentences/concepts\n",
    "- **Trade-off**: More overlap = better context but more redundancy\n",
    "- **Rule of thumb**: 10% of chunk size (e.g., 50 for size 512)\n",
    "\n",
    "**Recommendation**: 512 size + 50 overlap for balanced performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Similarity Top-K (1-20)\n",
    "**What it controls**: How many document chunks to retrieve.\n",
    "\n",
    "- **Lower (3-5)**:\n",
    "  - ‚úÖ Focused, faster\n",
    "  - ‚úÖ Lower cost\n",
    "  - ‚ùå May miss relevant info\n",
    "\n",
    "- **Higher (8-15)**:\n",
    "  - ‚úÖ More comprehensive\n",
    "  - ‚úÖ Less likely to miss relevant info\n",
    "  - ‚ùå Slower\n",
    "  - ‚ùå Higher cost\n",
    "  - ‚ùå More noise\n",
    "\n",
    "**Recommendation**: 5 for most queries, 10+ for complex analytical questions.\n",
    "\n",
    "---\n",
    "\n",
    "### Node Postprocessors\n",
    "**What they do**: Filter/rerank retrieved chunks before sending to LLM.\n",
    "\n",
    "**SimilarityPostprocessor**:\n",
    "- Removes chunks below similarity cutoff\n",
    "- ‚úÖ Improves quality (removes noise)\n",
    "- ‚úÖ Reduces cost (fewer tokens)\n",
    "- Works with similarity cutoff slider\n",
    "\n",
    "**Recommendation**: Enable for production use.\n",
    "\n",
    "---\n",
    "\n",
    "### Similarity Cutoff (0.0-1.0)\n",
    "**What it controls**: Minimum relevance score for postprocessor.\n",
    "\n",
    "- **Lower (0.1-0.3)**:\n",
    "  - ‚úÖ More permissive\n",
    "  - ‚úÖ Includes potentially relevant docs\n",
    "  - ‚ùå More noise\n",
    "\n",
    "- **Higher (0.5-0.8)**:\n",
    "  - ‚úÖ Only highly relevant docs\n",
    "  - ‚úÖ Cleaner results\n",
    "  - ‚ùå May filter out useful info\n",
    "\n",
    "**Recommendation**: 0.3 as default, adjust based on results.\n",
    "\n",
    "---\n",
    "\n",
    "### Response Synthesizers\n",
    "**What they do**: Combine multiple chunks into final answer.\n",
    "\n",
    "**Default**:\n",
    "- ‚úÖ Fast\n",
    "- ‚úÖ Simple\n",
    "- Good for straightforward queries\n",
    "\n",
    "**TreeSummarize**:\n",
    "- Hierarchical summarization\n",
    "- ‚úÖ Best for complex analytical queries\n",
    "- ‚úÖ Comprehensive answers\n",
    "- ‚ùå Slower (more API calls)\n",
    "- ‚ùå Higher cost\n",
    "\n",
    "**Refine**:\n",
    "- Iterative improvement\n",
    "- ‚úÖ Detailed, thorough answers\n",
    "- ‚úÖ Good for building on information\n",
    "- ‚ùå Slowest\n",
    "- ‚ùå Highest cost\n",
    "\n",
    "**CompactAndRefine**:\n",
    "- Balanced version of Refine\n",
    "- ‚úÖ Better than Default\n",
    "- ‚úÖ Faster than Refine\n",
    "- Good middle ground\n",
    "\n",
    "**Recommendation**: Default for speed, TreeSummarize for quality, CompactAndRefine for balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Assignment Completion Checklist\n",
    "\n",
    "### Implementation:\n",
    "- [x] API key configuration added\n",
    "- [x] Advanced RAG backend with all methods implemented\n",
    "- [x] Gradio interface with all required components:\n",
    "  - [x] Initialize database button\n",
    "  - [x] Model selection dropdown\n",
    "  - [x] Temperature slider\n",
    "  - [x] Chunk size input\n",
    "  - [x] Chunk overlap input\n",
    "  - [x] Similarity top-k slider\n",
    "  - [x] Node postprocessor checkbox\n",
    "  - [x] Similarity cutoff slider\n",
    "  - [x] Response synthesizer dropdown\n",
    "  - [x] Query input and submit button\n",
    "  - [x] Response output\n",
    "  - [x] Configuration display\n",
    "- [x] All components connected to backend functions\n",
    "- [x] Professional 2-column layout\n",
    "\n",
    "### Testing:\n",
    "- [ ] Database initialization works\n",
    "- [ ] All configuration controls update correctly\n",
    "- [ ] Queries return responses\n",
    "- [ ] Configuration display shows current settings\n",
    "- [ ] Tested different models\n",
    "- [ ] Tested different temperatures\n",
    "- [ ] Tested different chunk sizes\n",
    "- [ ] Tested different synthesizers\n",
    "- [ ] Tested postprocessor filtering\n",
    "\n",
    "### Understanding:\n",
    "- [ ] Understand how each parameter affects results\n",
    "- [ ] Can explain model differences\n",
    "- [ ] Can explain temperature effects\n",
    "- [ ] Can explain chunking strategies\n",
    "- [ ] Can explain synthesizer differences\n",
    "- [ ] Can explain postprocessor benefits\n",
    "\n",
    "---\n",
    "\n",
    "## üéä Congratulations!\n",
    "\n",
    "You've successfully built a **professional, production-ready RAG application**! \n",
    "\n",
    "### What You Achieved:\n",
    "‚úÖ **Full configurability** - Every RAG parameter exposed and adjustable\n",
    "‚úÖ **Professional UI** - Clean 2-column layout with organized controls\n",
    "‚úÖ **Real-time configuration** - Experiment with settings and see immediate results\n",
    "‚úÖ **Production patterns** - Error handling, validation, configuration display\n",
    "‚úÖ **Advanced features** - Multiple models, synthesizers, postprocessors\n",
    "\n",
    "### Skills Mastered:\n",
    "- Building complex Gradio interfaces with multiple components\n",
    "- Dynamic RAG configuration and parameter tuning\n",
    "- Professional UI/UX design patterns\n",
    "- Production-ready error handling\n",
    "- Performance vs quality trade-offs\n",
    "\n",
    "### What Makes This Production-Ready:\n",
    "1. **Comprehensive Configuration** - All parameters tunable\n",
    "2. **Error Handling** - Graceful failures with user-friendly messages\n",
    "3. **Transparency** - Shows exact configuration used for each query\n",
    "4. **Flexibility** - Supports multiple models and strategies\n",
    "5. **Professional Design** - Clean, organized, intuitive interface\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps & Career Applications\n",
    "\n",
    "### Immediate Enhancements:\n",
    "1. **Save/Load Configs** - Store favorite configurations\n",
    "2. **Comparison Mode** - Side-by-side results with different configs\n",
    "3. **Cost Tracking** - Monitor API costs per query\n",
    "4. **Performance Metrics** - Track response times\n",
    "5. **Export Results** - Download responses as markdown/PDF\n",
    "\n",
    "### Production Deployment:\n",
    "- **Hugging Face Spaces** - Free hosting with GPU support\n",
    "- **Docker** - Containerize for scalability\n",
    "- **Cloud Platforms** - AWS/GCP/Azure deployment\n",
    "- **Authentication** - Add user accounts\n",
    "- **Database** - Store queries and configurations\n",
    "\n",
    "### Portfolio Value:\n",
    "This project demonstrates:\n",
    "- ‚úÖ Advanced AI/ML application development\n",
    "- ‚úÖ Production-ready code quality\n",
    "- ‚úÖ Full-stack capabilities (backend + frontend)\n",
    "- ‚úÖ Understanding of RAG systems\n",
    "- ‚úÖ Professional UI/UX design\n",
    "\n",
    "### Interview Talking Points:\n",
    "- \"Built a configurable RAG system with 8+ tunable parameters\"\n",
    "- \"Implemented multiple response synthesis strategies\"\n",
    "- \"Created professional web UI with Gradio for ML applications\"\n",
    "- \"Optimized for cost vs quality trade-offs\"\n",
    "- \"Production-ready with error handling and validation\"\n",
    "\n",
    "---\n",
    "\n",
    "**You're now equipped to build sophisticated AI applications!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
