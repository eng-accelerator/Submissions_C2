{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e9123a",
   "metadata": {},
   "source": [
    "# RAG Implementation with LlamaIndex and LanceDB\n",
    "\n",
    "This notebook demonstrates a complete RAG (Retrieval Augmented Generation) implementation using LlamaIndex and LanceDB. We'll explore three different approaches:\n",
    "\n",
    "1. **Vector Search Only** - Fast retrieval without LLM generation\n",
    "2. **HuggingFace API Integration** - Cloud-based LLM with authentication\n",
    "3. **Local LLM with Ollama** - Complete local solution\n",
    "\n",
    "## Overview\n",
    "\n",
    "The notebook covers:\n",
    "- Data loading and preparation from HuggingFace datasets\n",
    "- Vector store setup with LanceDB\n",
    "- Embedding generation with HuggingFace models\n",
    "- Three different query approaches with increasing complexity\n",
    "- Utility functions for table exploration and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf9504",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc3a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install all required packages\n",
    "!pip install llama-index llama-index-vector-stores-lancedb llama-index-embeddings-huggingface llama-index-llms-huggingface-api lancedb datasets -q\n",
    "\n",
    "# # Additional packages for local LLM and utilities\n",
    "!pip install llama-index-llms-ollama requests -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf987ff-9e61-4e58-85d1-0d69671f0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b51b6153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lancedb\n",
    "import subprocess\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "# LlamaIndex core components\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# Embedding and vector store\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "# LLM integrations\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Async support for notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6297a29",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df8ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 personas from dataset...\n",
      "Prepared 100 documents\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(num_samples=100):\n",
    "    \"\"\"\n",
    "    Load dataset and create document files\n",
    "    \"\"\"\n",
    "    print(f\"Loading {num_samples} personas from dataset...\")\n",
    "    \n",
    "    # Load the personas dataset\n",
    "    dataset = load_dataset(\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
    "    \n",
    "    # Create data directory\n",
    "    Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save personas as text files and create Document objects\n",
    "    documents = []\n",
    "    for i, persona in enumerate(dataset.select(range(min(num_samples, len(dataset))))):\n",
    "        # Create Document objects for LlamaIndex\n",
    "        doc = Document(\n",
    "            text=persona[\"persona\"],\n",
    "            metadata={\n",
    "                \"persona_id\": i,\n",
    "                \"source\": \"finepersonas-dataset\"\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        \n",
    "        # Optionally save to files as well\n",
    "        with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(persona[\"persona\"])\n",
    "    \n",
    "    print(f\"Prepared {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "# Load the data\n",
    "documents = prepare_data(num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def1b98",
   "metadata": {},
   "source": [
    "## 4. LanceDB Vector Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6741ba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LanceDB connection...\n",
      "Connected to LanceDB, table: personas_rag\n"
     ]
    }
   ],
   "source": [
    "def setup_lancedb_store(table_name=\"personas_rag\"):\n",
    "    \"\"\"\n",
    "    Initialize LanceDB and create/connect to a table\n",
    "    \"\"\"\n",
    "    print(\"Setting up LanceDB connection...\")\n",
    "    \n",
    "    # Create or connect to LanceDB\n",
    "    db = lancedb.connect(\"./lancedb_data\")\n",
    "    \n",
    "    # LlamaIndex will handle table creation with proper schema\n",
    "    print(f\"Connected to LanceDB, table: {table_name}\")\n",
    "    \n",
    "    return db, table_name\n",
    "\n",
    "# Setup database connection\n",
    "db, table_name = setup_lancedb_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d2ce0",
   "metadata": {},
   "source": [
    "## 5. Vector Embeddings and Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ec8a431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding model and ingestion pipeline...\n",
      "Processing documents and creating embeddings...\n",
      "Successfully processed 100 text chunks\n"
     ]
    }
   ],
   "source": [
    "async def create_and_populate_index(documents, db, table_name):\n",
    "    \"\"\"\n",
    "    Create ingestion pipeline and populate LanceDB with embeddings\n",
    "    \"\"\"\n",
    "    print(\"Creating embedding model and ingestion pipeline...\")\n",
    "    \n",
    "    # Initialize embedding model\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=\"BAAI/bge-small-en-v1.5\"\n",
    "    )\n",
    "    \n",
    "    # Create LanceDB vector store\n",
    "    vector_store = LanceDBVectorStore(\n",
    "        uri=\"./lancedb_data\",\n",
    "        table_name=table_name,\n",
    "        mode=\"overwrite\"  # overwrite existing table\n",
    "    )\n",
    "    \n",
    "    # Create ingestion pipeline\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
    "            embed_model,\n",
    "        ],\n",
    "        vector_store=vector_store,\n",
    "    )\n",
    "    \n",
    "    print(\"Processing documents and creating embeddings...\")\n",
    "    # Run the pipeline to process documents and store in LanceDB\n",
    "    nodes = await pipeline.arun(documents=documents)\n",
    "    print(f\"Successfully processed {len(nodes)} text chunks\")\n",
    "    \n",
    "    return vector_store, embed_model\n",
    "\n",
    "# Create embeddings and populate vector store\n",
    "vector_store, embed_model = await create_and_populate_index(documents, db, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd7550",
   "metadata": {},
   "source": [
    "## 6. Option 1: Vector Search Only (No LLM)\n",
    "\n",
    "This approach provides fast document retrieval without LLM generation. Perfect for finding relevant content quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b00c89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Vector Search (No LLM needed)\n",
      "==================================================\n",
      "\n",
      "Query: technology and artificial intelligence expert\n",
      "------------------------------\n",
      "\n",
      "Result 1 (Score: 0.589):\n",
      "A computer scientist or electronics engineer researching alternative sustainable materials for neuromorphic computing and memristor development or A science journalist covering emerging technologies a...\n",
      "\n",
      "Result 2 (Score: 0.589):\n",
      "A computer scientist or electronics engineer researching alternative sustainable materials for neuromorphic computing and memristor development or A science journalist covering emerging technologies a...\n",
      "\n",
      "Result 3 (Score: 0.589):\n",
      "A computer scientist or electronics engineer researching alternative sustainable materials for neuromorphic computing and memristor development or A science journalist covering emerging technologies a...\n",
      "\n",
      "Query: teacher educator professor\n",
      "------------------------------\n",
      "\n",
      "Result 1 (Score: 0.577):\n",
      "An English language arts teacher with a focus on upper elementary education....\n",
      "\n",
      "Result 2 (Score: 0.577):\n",
      "An English language arts teacher with a focus on upper elementary education....\n",
      "\n",
      "Result 3 (Score: 0.577):\n",
      "An English language arts teacher with a focus on upper elementary education....\n",
      "\n",
      "Query: environment climate sustainability\n",
      "------------------------------\n",
      "\n",
      "Result 1 (Score: 0.683):\n",
      "An environmental scientist focused on climate change and pollution issues, or a sustainability advocate pushing for global action on reducing greenhouse gas emissions....\n",
      "\n",
      "Result 2 (Score: 0.683):\n",
      "An environmental scientist focused on climate change and pollution issues, or a sustainability advocate pushing for global action on reducing greenhouse gas emissions....\n",
      "\n",
      "Result 3 (Score: 0.683):\n",
      "An environmental scientist focused on climate change and pollution issues, or a sustainability advocate pushing for global action on reducing greenhouse gas emissions....\n",
      "\n",
      "Query: art culture heritage creative\n",
      "------------------------------\n",
      "\n",
      "Result 1 (Score: 0.625):\n",
      "A local art historian and museum professional interested in 19th-century American art and the local cultural heritage of Cincinnati....\n",
      "\n",
      "Result 2 (Score: 0.625):\n",
      "A local art historian and museum professional interested in 19th-century American art and the local cultural heritage of Cincinnati....\n",
      "\n",
      "Result 3 (Score: 0.625):\n",
      "A local art historian and museum professional interested in 19th-century American art and the local cultural heritage of Cincinnati....\n"
     ]
    }
   ],
   "source": [
    "def perform_vector_search(db, table_name, query_text, embed_model, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform direct vector search on LanceDB\n",
    "    \"\"\"\n",
    "    # Get query embedding\n",
    "    query_embedding = embed_model.get_text_embedding(query_text)\n",
    "    \n",
    "    # Open table and perform search\n",
    "    table = db.open_table(table_name)\n",
    "    results = table.search(query_embedding).limit(top_k).to_pandas()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_vector_search():\n",
    "    \"\"\"\n",
    "    Test vector search functionality with sample queries\n",
    "    \"\"\"\n",
    "    print(\"Testing Vector Search (No LLM needed)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test queries\n",
    "    queries = [\n",
    "        \"technology and artificial intelligence expert\",\n",
    "        \"teacher educator professor\",\n",
    "        \"environment climate sustainability\", \n",
    "        \"art culture heritage creative\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Perform search\n",
    "        results = perform_vector_search(db, table_name, query, embed_model, top_k=3)\n",
    "        \n",
    "        for idx, row in results.iterrows():\n",
    "            score = row.get('_distance', 'N/A')\n",
    "            text = row.get('text', 'N/A')\n",
    "            \n",
    "            # Format score\n",
    "            if isinstance(score, (int, float)):\n",
    "                score_str = f\"{score:.3f}\"\n",
    "            else:\n",
    "                score_str = str(score)\n",
    "            \n",
    "            print(f\"\\nResult {idx + 1} (Score: {score_str}):\")\n",
    "            print(f\"{text[:200]}...\")\n",
    "\n",
    "# Run vector search test\n",
    "test_vector_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7668d",
   "metadata": {},
   "source": [
    "## 7. Option 2: RAG with HuggingFace API\n",
    "\n",
    "This approach uses HuggingFace's cloud API for LLM generation. Requires API token authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b797dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG with HuggingFace API\n",
      "========================================\n",
      "\n",
      "Query: Find personas interested in technology and AI\n",
      "------------------------------\n",
      "Error: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta/v1/chat/completions (Request ID: Root=1-6905c66a-79128e1229545d9f5beb4d96;558cc579-5859-48b0-b07a-f042c443b18c)\n",
      "\n",
      "Query: Who are the educators or teachers in the dataset?\n",
      "------------------------------\n",
      "Error: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta/v1/chat/completions (Request ID: Root=1-6905c66b-0d00822f0aed3c0f19a33280;2a27c619-997f-4200-8109-88896d96d101)\n",
      "\n",
      "Query: Describe personas working with environmental topics\n",
      "------------------------------\n",
      "Error: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta/v1/chat/completions (Request ID: Root=1-6905c66c-262ae12d2152188d6e697a37;9dcd90a7-1795-41c8-aee1-27ae0b8eb50f)\n"
     ]
    }
   ],
   "source": [
    "# Set your HuggingFace API token here\n",
    "# Get your free token from: https://huggingface.co/settings/tokens\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"your_token"\"  # Replace with your actual token\n",
    "\n",
    "def create_query_engine(vector_store, embed_model, llm=None):\n",
    "    \"\"\"\n",
    "    Create a query engine from the vector store\n",
    "    \"\"\"\n",
    "    # Create index from vector store\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "    \n",
    "    # Setup LLM if provided\n",
    "    query_engine_kwargs = {}\n",
    "    if llm:\n",
    "        query_engine_kwargs['llm'] = llm\n",
    "    \n",
    "    # Create query engine\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        **query_engine_kwargs\n",
    "    )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "def query_rag(query_engine, question):\n",
    "    \"\"\"\n",
    "    Query the RAG system and return response\n",
    "    \"\"\"\n",
    "    response = query_engine.query(question)\n",
    "    return response\n",
    "\n",
    "async def test_huggingface_rag():\n",
    "    \"\"\"\n",
    "    Test RAG with HuggingFace API\n",
    "    \"\"\"\n",
    "    print(\"Testing RAG with HuggingFace API\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Initialize HuggingFace LLM with authentication\n",
    "        llm = HuggingFaceInferenceAPI(\n",
    "            model_name= \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "            token=os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        # Create query engine\n",
    "        query_engine = create_query_engine(vector_store, embed_model, llm)\n",
    "        \n",
    "        # Test queries\n",
    "        queries = [\n",
    "            \"Find personas interested in technology and AI\",\n",
    "            \"Who are the educators or teachers in the dataset?\",\n",
    "            \"Describe personas working with environmental topics\"\n",
    "        ]\n",
    "        \n",
    "        for query in queries:\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            try:\n",
    "                response = query_rag(query_engine, query)\n",
    "                print(f\"Response: {response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Setup error: {e}\")\n",
    "        print(\"Make sure to set your HuggingFace API token above\")\n",
    "\n",
    "# Uncomment the line below after setting your API token\n",
    "await test_huggingface_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c437ab16",
   "metadata": {},
   "source": [
    "## 8. Option 3: RAG with Local LLM (Ollama)\n",
    "\n",
    "This approach uses a completely local LLM setup. No internet required after initial setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3744e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is not installed or not in PATH\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_ollama_installed():\n",
    "    \"\"\"Check if Ollama is installed\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run([\"ollama\", \"--version\"], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Ollama is installed: {result.stdout.strip()}\")\n",
    "            return True\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    print(\"Ollama is not installed or not in PATH\")\n",
    "    return False\n",
    "\n",
    "def download_ollama():\n",
    "    \"\"\"Download Ollama installer for Windows\"\"\"\n",
    "    print(\"Downloading Ollama for Windows...\")\n",
    "    \n",
    "    url = \"https://ollama.com/download/OllamaSetup.exe\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    installer_path = Path(\"OllamaSetup.exe\")\n",
    "    with open(installer_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    print(\"Ollama downloaded successfully!\")\n",
    "    print(\"Please run the installer manually and then continue.\")\n",
    "    print(f\"Installer location: {installer_path.absolute()}\")\n",
    "    \n",
    "    return installer_path\n",
    "\n",
    "def start_ollama_service():\n",
    "    \"\"\"Start Ollama service\"\"\"\n",
    "    try:\n",
    "        print(\"Starting Ollama service...\")\n",
    "        subprocess.Popen([\"ollama\", \"serve\"], shell=True)\n",
    "        time.sleep(3)\n",
    "        print(\"Ollama service started!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to start Ollama: {e}\")\n",
    "        return False\n",
    "\n",
    "def pull_ollama_model(model_name=\"llama3.2:1b\"):\n",
    "    \"\"\"Pull a lightweight model for local inference\"\"\"\n",
    "    try:\n",
    "        print(f\"Pulling model: {model_name}\")\n",
    "        result = subprocess.run([\"ollama\", \"pull\", model_name], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Model {model_name} pulled successfully!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to pull model: {result.stderr}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error pulling model: {e}\")\n",
    "        return False\n",
    "\n",
    "def setup_ollama():\n",
    "    \"\"\"Complete Ollama setup\"\"\"\n",
    "    if not check_ollama_installed():\n",
    "        print(\"Ollama needs to be installed.\")\n",
    "        download_ollama()\n",
    "        return False\n",
    "    \n",
    "    if not start_ollama_service():\n",
    "        return False\n",
    "    \n",
    "    if not pull_ollama_model(\"llama3.2:1b\"):\n",
    "        return False\n",
    "    \n",
    "    print(\"Ollama setup complete!\")\n",
    "    return True\n",
    "\n",
    "# Check Ollama installation\n",
    "check_ollama_installed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d2011bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG with Local LLM (Ollama)\n",
      "========================================\n",
      "\n",
      "Query: Find personas interested in technology and AI\n",
      "------------------------------\n",
      "Response: People interested in technology and AI.\n",
      "\n",
      "Query: Who are the educators or teachers in the dataset?\n",
      "------------------------------\n",
      "Response: The dataset contains information about English language arts teachers.\n",
      "\n",
      "Query: Describe personas working with environmental topics\n",
      "------------------------------\n",
      "Response: Environmental scientists who specialize in climate change and pollution issues, or sustainability advocates advocating for global emissions reductions, are the target group.\n"
     ]
    }
   ],
   "source": [
    "async def test_local_llm_rag():\n",
    "    \"\"\"\n",
    "    Test RAG with local Ollama LLM\n",
    "    \"\"\"\n",
    "    print(\"Testing RAG with Local LLM (Ollama)\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Initialize local Ollama LLM\n",
    "        llm = Ollama(\n",
    "            model=\"gemma3:1b\",\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            request_timeout=60.0\n",
    "        )\n",
    "        \n",
    "        # Create query engine\n",
    "        query_engine = create_query_engine(vector_store, embed_model, llm)\n",
    "        \n",
    "        # Test queries\n",
    "        queries = [\n",
    "            \"Find personas interested in technology and AI\",\n",
    "            \"Who are the educators or teachers in the dataset?\",\n",
    "            \"Describe personas working with environmental topics\"\n",
    "        ]\n",
    "        \n",
    "        for query in queries:\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            try:\n",
    "                response = query_rag(query_engine, query)\n",
    "                print(f\"Response: {response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                print(\"Make sure Ollama is running with: ollama serve\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Setup error: {e}\")\n",
    "        print(\"Make sure Ollama is installed and running\")\n",
    "\n",
    "# Uncomment after Ollama setup is complete\n",
    "await test_local_llm_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c42b76",
   "metadata": {},
   "source": [
    "## 9. Utility Functions and Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a81e5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage Examples:\n",
      "==============================\n",
      "\n",
      "1. Vector Search Only:\n",
      "   test_vector_search()\n",
      "\n",
      "2. HuggingFace API RAG:\n",
      "   # Set API token first\n",
      "   os.environ['HUGGINGFACE_API_KEY'] = 'your_token'\n",
      "   await test_huggingface_rag()\n",
      "\n",
      "3. Local LLM RAG:\n",
      "   # Install and setup Ollama first\n",
      "   setup_ollama()\n",
      "   await test_local_llm_rag()\n",
      "\n",
      "4. Explore Database:\n",
      "   explore_lancedb_table(db, table_name)\n"
     ]
    }
   ],
   "source": [
    "def explore_lancedb_table(db, table_name):\n",
    "    \"\"\"\n",
    "    Explore the structure and content of the LanceDB table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table = db.open_table(table_name)\n",
    "        \n",
    "        print(\"Table Schema:\")\n",
    "        print(table.schema)\n",
    "        \n",
    "        print(f\"\\nTotal records: {table.count_rows()}\")\n",
    "        \n",
    "        print(\"\\nSample records:\")\n",
    "        df = table.to_pandas().head()\n",
    "        print(df)\n",
    "        \n",
    "        return table\n",
    "    except Exception as e:\n",
    "        print(f\"Error exploring table: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_filtered_query_engine(db, table_name, embed_model, filter_dict=None):\n",
    "    \"\"\"\n",
    "    Create a query engine with metadata filtering capabilities\n",
    "    \"\"\"\n",
    "    from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n",
    "    \n",
    "    # Reconnect to existing table\n",
    "    vector_store = LanceDBVectorStore(\n",
    "        uri=\"./lancedb_data\",\n",
    "        table_name=table_name,\n",
    "        mode=\"read\"\n",
    "    )\n",
    "    \n",
    "    # Create index\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "    \n",
    "    # Create query engine with filters if provided\n",
    "    if filter_dict:\n",
    "        filters = MetadataFilters(\n",
    "            filters=[\n",
    "                MetadataFilter(\n",
    "                    key=key,\n",
    "                    value=value,\n",
    "                    operator=FilterOperator.EQ\n",
    "                ) for key, value in filter_dict.items()\n",
    "            ]\n",
    "        )\n",
    "        query_engine = index.as_query_engine(\n",
    "            filters=filters,\n",
    "            response_mode=\"tree_summarize\"\n",
    "        )\n",
    "    else:\n",
    "        query_engine = index.as_query_engine(\n",
    "            response_mode=\"tree_summarize\"\n",
    "        )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "async def batch_process_documents(documents, batch_size=50):\n",
    "    \"\"\"\n",
    "    Process documents in batches for large datasets\n",
    "    \"\"\"\n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    \n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        table_name = f\"personas_batch_{i//batch_size}\"\n",
    "        \n",
    "        vector_store = LanceDBVectorStore(\n",
    "            uri=\"./lancedb_data\",\n",
    "            table_name=table_name,\n",
    "            mode=\"overwrite\"\n",
    "        )\n",
    "        \n",
    "        pipeline = IngestionPipeline(\n",
    "            transformations=[\n",
    "                SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
    "                embed_model,\n",
    "            ],\n",
    "            vector_store=vector_store,\n",
    "        )\n",
    "        \n",
    "        nodes = await pipeline.arun(documents=batch)\n",
    "        print(f\"Processed batch {i//batch_size + 1}: {len(nodes)} nodes\")\n",
    "\n",
    "def show_usage_examples():\n",
    "    \"\"\"\n",
    "    Display usage examples for different scenarios\n",
    "    \"\"\"\n",
    "    print(\"Usage Examples:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    print(\"\\n1. Vector Search Only:\")\n",
    "    print(\"   test_vector_search()\")\n",
    "    \n",
    "    print(\"\\n2. HuggingFace API RAG:\")\n",
    "    print(\"   # Set API token first\")\n",
    "    print(\"   os.environ['HUGGINGFACE_API_KEY'] = 'your_token'\")\n",
    "    print(\"   await test_huggingface_rag()\")\n",
    "    \n",
    "    print(\"\\n3. Local LLM RAG:\")\n",
    "    print(\"   # Install and setup Ollama first\")\n",
    "    print(\"   setup_ollama()\")\n",
    "    print(\"   await test_local_llm_rag()\")\n",
    "    \n",
    "    print(\"\\n4. Explore Database:\")\n",
    "    print(\"   explore_lancedb_table(db, table_name)\")\n",
    "\n",
    "# Show usage examples\n",
    "show_usage_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60273d9b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides three complete RAG implementation approaches:\n",
    "\n",
    "### Option 1: Vector Search Only\n",
    "- **Best for**: Fast document retrieval, no generation needed\n",
    "- **Advantages**: Very fast, no API costs, no setup complexity\n",
    "- **Use case**: Finding relevant documents, initial exploration\n",
    "\n",
    "### Option 2: HuggingFace API\n",
    "- **Best for**: High-quality responses with cloud LLMs\n",
    "- **Advantages**: Latest models, no local resources needed\n",
    "- **Requirements**: HuggingFace API token\n",
    "- **Use case**: Production applications with budget for API calls\n",
    "\n",
    "### Option 3: Local LLM (Ollama)\n",
    "- **Best for**: Complete privacy, no internet dependency\n",
    "- **Advantages**: No API costs, full control, offline capability\n",
    "- **Requirements**: Ollama installation, local compute resources\n",
    "- **Use case**: Private data, cost-sensitive applications\n",
    "\n",
    "Choose the approach that best fits your needs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
