{"cells":[{"cell_type":"markdown","metadata":{"id":"BszqGae1S-U9"},"source":["# Assignment 3b: Advanced Gradio RAG Frontend\n","## Day 6 Session 2 - Building Configurable RAG Applications\n","\n","In this assignment, you'll extend your basic RAG interface with advanced configuration options to create a professional, feature-rich RAG application.\n","\n","**New Features to Add:**\n","- Model selection dropdown (gpt-4o, gpt-4o-mini)\n","- Temperature slider (0 to 1 with 0.1 intervals)\n","- Chunk size configuration\n","- Chunk overlap configuration  \n","- Similarity top-k slider\n","- Node postprocessor multiselect\n","- Similarity cutoff slider\n","- Response synthesizer multiselect\n","\n","**Learning Objectives:**\n","- Advanced Gradio components and interactions\n","- Dynamic RAG configuration\n","- Professional UI design patterns\n","- Parameter validation and handling\n","- Building production-ready AI applications\n","\n","**Prerequisites:**\n","- Completed Assignment 3a (Basic Gradio RAG)\n","- Understanding of RAG parameters and their effects\n"]},{"cell_type":"markdown","metadata":{"id":"t-32uIGcS-U-"},"source":["## üìö Part 1: Setup and Imports\n","\n","Import all necessary libraries including advanced RAG components for configuration options.\n","\n","**Note:** This assignment uses OpenRouter for LLM access (not OpenAI). Make sure you have your `OPENROUTER_API_KEY` environment variable set.\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8R1DZvC3K67","executionInfo":{"status":"ok","timestamp":1762080202650,"user_tz":-330,"elapsed":24365,"user":{"displayName":"Soumya S","userId":"13030035295597085735"}},"outputId":"2069ac5b-a540-4045-88a1-ca1d8c41dd77"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","from getpass import getpass\n","\n","# securely input your key\n","os.environ[\"OPENROUTER_API_KEY\"] = getpass(\"Enter your OpenRouter key\")\n","print(\"‚úì OpenrRouter key set successfully\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BOjph4fp3N-y","executionInfo":{"status":"ok","timestamp":1762080215869,"user_tz":-330,"elapsed":7910,"user":{"displayName":"Soumya S","userId":"13030035295597085735"}},"outputId":"e6c761c6-4211-482c-8fdf-8a6330caa1c4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter your OpenRouter key¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n","‚úì OpenrRouter key set successfully\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nTlk2riDS-U_","executionInfo":{"status":"ok","timestamp":1762080332897,"user_tz":-330,"elapsed":41485,"user":{"displayName":"Soumya S","userId":"13030035295597085735"}},"outputId":"a24b3644-d99d-4219-d2f1-cdbd102a490f"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ All libraries imported successfully!\n"]}],"source":["# Import all required libraries\n","import gradio as gr\n","import os\n","from pathlib import Path\n","from typing import Dict, List, Optional, Any\n","\n","# LlamaIndex core components\n","from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n","from llama_index.vector_stores.lancedb import LanceDBVectorStore\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.llms.openrouter import OpenRouter\n","\n","# Advanced RAG components\n","from llama_index.core.postprocessor import SimilarityPostprocessor\n","from llama_index.core.response_synthesizers import TreeSummarize, Refine, CompactAndRefine\n","from llama_index.core.retrievers import VectorIndexRetriever\n","\n","print(\"‚úÖ All libraries imported successfully!\")\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d783128e","executionInfo":{"status":"ok","timestamp":1762080277206,"user_tz":-330,"elapsed":33998,"user":{"displayName":"Soumya S","userId":"13030035295597085735"}},"outputId":"543ba9fe-e1a5-4efc-ee76-aaa4cfefc1f1"},"source":["%pip install llama-index llama-index-vector-stores-lancedb llama-index-embeddings-huggingface llama-index-llms-openrouter --quiet"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.7/38.7 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m48.0/48.0 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m229.6/229.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"markdown","metadata":{"id":"EV6ydeeWS-VD"},"source":["## ü§ñ Part 2: Advanced RAG Backend Class\n","\n","Create an advanced RAG backend that supports dynamic configuration of all parameters.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ehA210DS-VD","executionInfo":{"status":"ok","timestamp":1762080382504,"user_tz":-330,"elapsed":1691,"user":{"displayName":"Soumya S","userId":"13030035295597085735"}},"outputId":"5986f524-eb29-43dd-9147-71379d7f0827"},"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Advanced RAG Backend initialized and ready!\n"]}],"source":["class AdvancedRAGBackend:\n","    \"\"\"Advanced RAG backend with configurable parameters.\"\"\"\n","\n","    def __init__(self):\n","        self.index = None\n","        self.available_models = [\"gpt-4o\", \"gpt-4o-mini\"]\n","        self.available_postprocessors = [\"SimilarityPostprocessor\"]\n","        self.available_synthesizers = [\"TreeSummarize\", \"Refine\", \"CompactAndRefine\", \"Default\"]\n","        self.update_settings()\n","\n","    def update_settings(self, model: str = \"gpt-4o-mini\", temperature: float = 0.1, chunk_size: int = 512, chunk_overlap: int = 50):\n","        \"\"\"Update LlamaIndex settings based on user configuration.\"\"\"\n","        # Set up the LLM using OpenRouter\n","        api_key = os.getenv(\"OPENROUTER_API_KEY\")\n","        if api_key:\n","            Settings.llm = OpenRouter(\n","                api_key=api_key,\n","                model=model,\n","                temperature=temperature\n","            )\n","\n","        # Set up the embedding model (keep this constant)\n","        Settings.embed_model = HuggingFaceEmbedding(\n","            model_name=\"BAAI/bge-small-en-v1.5\",\n","            trust_remote_code=True\n","        )\n","\n","        # Set chunking parameters from function parameters\n","        Settings.chunk_size = chunk_size\n","        Settings.chunk_overlap = chunk_overlap\n","\n","    def initialize_database(self, data_folder=\"/content/drive/MyDrive/datafiles/travel\"):\n","        \"\"\"Initialize the vector database with documents.\"\"\"\n","        # Check if data folder exists\n","        if not Path(data_folder).exists():\n","            return f\"‚ùå Data folder '{data_folder}' not found!\"\n","\n","        try:\n","            # Create vector store\n","            vector_store = LanceDBVectorStore(\n","                uri=\"./advanced_rag_vectordb\",\n","                table_name=\"documents\"\n","            )\n","\n","            # Load documents\n","            reader = SimpleDirectoryReader(input_dir=data_folder, recursive=True)\n","            documents = reader.load_data()\n","\n","            # Create storage context and index\n","            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n","            self.index = VectorStoreIndex.from_documents(\n","                documents,\n","                storage_context=storage_context,\n","                show_progress=True\n","            )\n","\n","            return f\"‚úÖ Database initialized successfully with {len(documents)} documents!\"\n","\n","        except Exception as e:\n","            return f\"‚ùå Error initializing database: {str(e)}\"\n","\n","    def get_postprocessor(self, postprocessor_name: str, similarity_cutoff: float):\n","        \"\"\"Get the selected postprocessor.\"\"\"\n","        if postprocessor_name == \"SimilarityPostprocessor\":\n","            return SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n","        elif postprocessor_name == \"None\":\n","            return None\n","        else:\n","            return None\n","\n","    def get_synthesizer(self, synthesizer_name: str):\n","        \"\"\"Get the selected response synthesizer.\"\"\"\n","        if synthesizer_name == \"TreeSummarize\":\n","            return TreeSummarize()\n","        elif synthesizer_name == \"Refine\":\n","            return Refine()\n","        elif synthesizer_name == \"CompactAndRefine\":\n","            return CompactAndRefine()\n","        elif synthesizer_name == \"Default\":\n","            return None\n","        else:\n","            return None\n","\n","    def advanced_query(self, question: str, model: str, temperature: float,\n","                      chunk_size: int, chunk_overlap: int, similarity_top_k: int,\n","                      postprocessor_names: List[str], similarity_cutoff: float,\n","                      synthesizer_name: str) -> Dict[str, Any]:\n","        \"\"\"Query the RAG system with advanced configuration.\"\"\"\n","\n","        # Check if index exists\n","        if self.index is None:\n","            return {\"response\": \"‚ùå Please initialize the database first!\", \"sources\": [], \"config\": {}}\n","\n","        # Check if question is empty\n","        if not question or not question.strip():\n","            return {\"response\": \"‚ö†Ô∏è Please enter a question first!\", \"sources\": [], \"config\": {}}\n","\n","        try:\n","            # Update settings with new parameters\n","            self.update_settings(model, temperature, chunk_size, chunk_overlap)\n","\n","            # Get postprocessors\n","            postprocessors = []\n","            for name in postprocessor_names:\n","                processor = self.get_postprocessor(name, similarity_cutoff)\n","                if processor is not None:\n","                    postprocessors.append(processor)\n","\n","            # Get synthesizer\n","            synthesizer = self.get_synthesizer(synthesizer_name)\n","\n","            # Create query engine with all parameters\n","            query_engine_kwargs = {\"similarity_top_k\": similarity_top_k}\n","            if postprocessors:\n","                query_engine_kwargs[\"node_postprocessors\"] = postprocessors\n","            if synthesizer is not None:\n","                query_engine_kwargs[\"response_synthesizer\"] = synthesizer\n","\n","            query_engine = self.index.as_query_engine(**query_engine_kwargs)\n","\n","            # Query and get response\n","            response = query_engine.query(question)\n","\n","            # Extract source information if available\n","            sources = []\n","            if hasattr(response, 'source_nodes'):\n","                for node in response.source_nodes:\n","                    sources.append({\n","                        \"text\": node.text[:200] + \"...\",\n","                        \"score\": getattr(node, 'score', 0.0),\n","                        \"source\": getattr(node.node, 'metadata', {}).get('file_name', 'Unknown')\n","                    })\n","\n","            return {\n","                \"response\": str(response),\n","                \"sources\": sources,\n","                \"config\": {\n","                    \"model\": model,\n","                    \"temperature\": temperature,\n","                    \"chunk_size\": chunk_size,\n","                    \"chunk_overlap\": chunk_overlap,\n","                    \"similarity_top_k\": similarity_top_k,\n","                    \"postprocessors\": postprocessor_names,\n","                    \"similarity_cutoff\": similarity_cutoff,\n","                    \"synthesizer\": synthesizer_name\n","                }\n","            }\n","\n","        except Exception as e:\n","            return {\"response\": f\"‚ùå Error processing query: {str(e)}\", \"sources\": [], \"config\": {}}\n","\n","# Initialize the backend\n","rag_backend = AdvancedRAGBackend()\n","print(\"üöÄ Advanced RAG Backend initialized and ready!\")\n"]},{"cell_type":"markdown","metadata":{"id":"oDugNFXdS-VG"},"source":["## üé® Part 3: Advanced Gradio Interface\n","\n","Create a sophisticated Gradio interface with all the configuration options specified:\n","1. Database initialization button\n","2. Search query input and button  \n","3. Model selection dropdown\n","4. Temperature slider\n","5. Chunk size and overlap inputs\n","6. Similarity top-k slider\n","7. Node postprocessor multiselect\n","8. Similarity cutoff slider\n","9. Response synthesizer multiselect\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GKeydNG4S-VG","executionInfo":{"status":"ok","timestamp":1762080389619,"user_tz":-330,"elapsed":210,"user":{"displayName":"Soumya S","userId":"13030035295597085735"}},"outputId":"e20358fe-181a-451f-bf20-8eb48dd4dd44"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Advanced RAG interface created successfully!\n"]}],"source":["def create_advanced_rag_interface():\n","    \"\"\"Create advanced RAG interface with full configuration options.\"\"\"\n","\n","    def initialize_db():\n","        \"\"\"Handle database initialization.\"\"\"\n","        return rag_backend.initialize_database()\n","\n","    def handle_advanced_query(question, model, temperature, chunk_size, chunk_overlap,\n","                             similarity_top_k, postprocessors, similarity_cutoff, synthesizer):\n","        \"\"\"Handle advanced RAG queries with all configuration options.\"\"\"\n","        result = rag_backend.advanced_query(\n","            question, model, temperature, chunk_size, chunk_overlap,\n","            similarity_top_k, postprocessors, similarity_cutoff, synthesizer\n","        )\n","\n","        # Format configuration for display\n","        config_text = f\"\"\"**Current Configuration:**\n","- Model: {result['config'].get('model', 'N/A')}\n","- Temperature: {result['config'].get('temperature', 'N/A')}\n","- Chunk Size: {result['config'].get('chunk_size', 'N/A')}\n","- Chunk Overlap: {result['config'].get('chunk_overlap', 'N/A')}\n","- Similarity Top-K: {result['config'].get('similarity_top_k', 'N/A')}\n","- Postprocessors: {', '.join(result['config'].get('postprocessors', []))}\n","- Similarity Cutoff: {result['config'].get('similarity_cutoff', 'N/A')}\n","- Synthesizer: {result['config'].get('synthesizer', 'N/A')}\"\"\"\n","\n","        return result[\"response\"], config_text\n","\n","    # TODO: Create the advanced interface structure\n","    # Hint: This interface needs more complex layout with configuration controls\n","\n","    with gr.Blocks(title=\"Advanced RAG Assistant\") as interface:\n","        # TODO: Add title and description\n","        # Hint: Use gr.Markdown() for formatted text\n","\n","        # Your title and description here:\n","        gr.Markdown(\"# ü§ñ Advanced RAG Assistant\")\n","        gr.Markdown(\"Configure all RAG parameters for optimal performance and experiment with different settings!\")\n","\n","\n","        # TODO: Add database initialization section\n","        # Hint: Use gr.Button() for initialization and gr.Textbox() for status\n","        init_btn = gr.Button(\"üîÑ Initialize Vector Database\", variant=\"primary\")\n","        status_output = gr.Textbox(label=\"Database Status\", lines=2, interactive=False)\n","\n","\n","        # TODO: Create main layout with columns\n","        # Hint: Configuration controls on left, query/response on right makes sense\n","        # Use gr.Row() and gr.Column() to organize this\n","\n","        with gr.Row():\n","            with gr.Column(scale=1):\n","\n","                gr.Markdown(\"### ‚öôÔ∏è RAG Configuration\")\n","\n","                # TODO: Model selection\n","                # Hint: Use gr.Dropdown() with choices=[\"gpt-4o\", \"gpt-4o-mini\"]\n","                model_dropdown = gr.Dropdown(\n","                    choices=[\"gpt-4o\", \"gpt-4o-mini\"],\n","                    value=\"gpt-4o-mini\",\n","                    label=\"LLM Model\"\n","                )\n","\n","\n","                # TODO: Temperature control\n","                # Hint: Use gr.Slider() with minimum=0.0, maximum=1.0, step=0.1, value=0.1\n","                temperature_slider = gr.Slider(\n","                    minimum=0.0, maximum=1.0, step=0.1, value=0.1,\n","                    label=\"Temperature (0=deterministic, 1=creative)\"\n","                )\n","\n","\n","                # TODO: Chunking parameters\n","                # Hint: Use gr.Number() for numeric inputs with default values\n","                chunk_size_input = gr.Number(\n","                    value=512, minimum=128, maximum=2048,\n","                    label=\"Chunk Size\"\n","                )\n","\n","                chunk_overlap_input = gr.Number(\n","                    value=50, minimum=0, maximum=200,\n","                    label=\"Chunk Overlap\"\n","                )\n","\n","\n","                # TODO: Retrieval parameters\n","                # Hint: Use gr.Slider() with minimum=1, maximum=20, step=1, value=5\n","                similarity_topk_slider = gr.Slider(\n","                    minimum=1, maximum=20, step=1, value=5,\n","                    label=\"Similarity Top-K (documents to retrieve)\"\n","                )\n","\n","\n","                # TODO: Postprocessor selection\n","                # Hint: Use gr.CheckboxGroup() with choices=[\"SimilarityPostprocessor\"]\n","                postprocessor_checkbox = gr.CheckboxGroup(\n","                    choices=[\"SimilarityPostprocessor\"],\n","                    value=[\"SimilarityPostprocessor\"],\n","                    label=\"Node Postprocessors\"\n","                )\n","\n","\n","                # TODO: Similarity filtering\n","                # Hint: Use gr.Slider() with minimum=0.0, maximum=1.0, step=0.1, value=0.3\n","                similarity_cutoff_slider = gr.Slider(\n","                    minimum=0.0, maximum=1.0, step=0.1, value=0.3,\n","                    label=\"Similarity Cutoff (0=permissive, 1=strict)\"\n","                )\n","\n","\n","\n","                # TODO: Response synthesizer\n","                # Hint: Use gr.Dropdown() with choices=[\"TreeSummarize\", \"Refine\", \"CompactAndRefine\", \"Default\"]\n","                synthesizer_dropdown = gr.Dropdown(\n","                    choices=[\"TreeSummarize\", \"Refine\", \"CompactAndRefine\", \"Default\"],\n","                    value=\"TreeSummarize\",\n","                    label=\"Response Synthesizer\"\n","                )\n","\n","\n","            with gr.Column(scale=2):\n","                gr.Markdown(\"### üí¨ Query Interface\")\n","\n","                # TODO: Query input\n","                # Hint: Use gr.Textbox() with label=\"Ask a question\", placeholder text, lines=3\n","                query_input = gr.Textbox(\n","                    label=\"Ask a question\",\n","                    placeholder=\"Enter your question about the documents...\",\n","                    lines=3\n","                )\n","\n","\n","                # TODO: Submit button\n","                # Hint: Use gr.Button() with variant=\"primary\"\n","                submit_btn = gr.Button(\"üöÄ Ask Question\", variant=\"primary\")\n","\n","\n","                # TODO: Response output\n","                # Hint: Use gr.Textbox() with lines=12, interactive=False\n","                response_output = gr.Textbox(\n","                    label=\"AI Response\",\n","                    lines=12,\n","                    interactive=False\n","                )\n","\n","\n","                # TODO: Configuration display\n","                # Hint: Use gr.Textbox() with lines=8, interactive=False\n","                config_display = gr.Textbox(\n","                    label=\"Configuration Used\",\n","                    lines=8,\n","                    interactive=False\n","                )\n","\n","\n","        # Uncomment to Connect functions to components\n","        init_btn.click(initialize_db, outputs=[status_output])\n","\n","        submit_btn.click(\n","          handle_advanced_query,\n","           inputs=[\n","                query_input, model_dropdown, temperature_slider,\n","               chunk_size_input, chunk_overlap_input, similarity_topk_slider,\n","               postprocessor_checkbox, similarity_cutoff_slider, synthesizer_dropdown\n","            ],\n","             outputs=[response_output, config_display]\n","         )\n","\n","\n","    return interface\n","\n","# Create the interface\n","advanced_interface = create_advanced_rag_interface()\n","print(\"‚úÖ Advanced RAG interface created successfully!\")\n"]},{"cell_type":"markdown","metadata":{"id":"9n_hLuHDS-VJ"},"source":["## üöÄ Part 4: Launch Your Advanced Application\n","\n","Launch your advanced Gradio application and test all the configuration options!\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"_fdIXbF8S-VJ","executionInfo":{"status":"ok","timestamp":1762080395460,"user_tz":-330,"elapsed":1545,"user":{"displayName":"Soumya S","userId":"13030035295597085735"}},"outputId":"38ef6a6c-abf5-46f0-ed0b-fc8a7ad06396"},"outputs":[{"output_type":"stream","name":"stdout","text":["üéâ Launching your Advanced RAG Assistant...\n","üîó Your application will open in a new browser tab!\n","\n","‚ö†Ô∏è  Make sure your OPENROUTER_API_KEY environment variable is set!\n","\n","üìã Testing Instructions:\n","1. Click 'Initialize Vector Database' button first\n","2. Wait for success message\n","3. Configure your RAG parameters:\n","   - Choose model (gpt-4o, gpt-4o-mini)\n","   - Adjust temperature (0.0 = deterministic, 1.0 = creative)\n","   - Set chunk size and overlap\n","   - Choose similarity top-k\n","   - Select postprocessors and synthesizer\n","4. Enter a question and click 'Ask Question'\n","5. Review both the response and configuration used\n","\n","üß™ Experiments to try:\n","- Compare different models with the same question\n","- Test temperature effects (0.1 vs 0.9)\n","- Try different chunk sizes (256 vs 1024)\n","- Compare synthesizers (TreeSummarize vs Refine)\n","- Adjust similarity cutoff to filter results\n","It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://7369d366fce313241c.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://7369d366fce313241c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":9}],"source":["print(\"üéâ Launching your Advanced RAG Assistant...\")\n","print(\"üîó Your application will open in a new browser tab!\")\n","print(\"\")\n","print(\"‚ö†Ô∏è  Make sure your OPENROUTER_API_KEY environment variable is set!\")\n","print(\"\")\n","print(\"üìã Testing Instructions:\")\n","print(\"1. Click 'Initialize Vector Database' button first\")\n","print(\"2. Wait for success message\")\n","print(\"3. Configure your RAG parameters:\")\n","print(\"   - Choose model (gpt-4o, gpt-4o-mini)\")\n","print(\"   - Adjust temperature (0.0 = deterministic, 1.0 = creative)\")\n","print(\"   - Set chunk size and overlap\")\n","print(\"   - Choose similarity top-k\")\n","print(\"   - Select postprocessors and synthesizer\")\n","print(\"4. Enter a question and click 'Ask Question'\")\n","print(\"5. Review both the response and configuration used\")\n","print(\"\")\n","print(\"üß™ Experiments to try:\")\n","print(\"- Compare different models with the same question\")\n","print(\"- Test temperature effects (0.1 vs 0.9)\")\n","print(\"- Try different chunk sizes (256 vs 1024)\")\n","print(\"- Compare synthesizers (TreeSummarize vs Refine)\")\n","print(\"- Adjust similarity cutoff to filter results\")\n","\n","# Your code here:\n","advanced_interface.launch()"]},{"cell_type":"markdown","metadata":{"id":"nj9KIWbkS-VL"},"source":["## üí° Understanding the Configuration Options\n","\n","### Model Selection\n","- **gpt-4o**: Latest and most capable model, best quality responses\n","- **gpt-4o-mini**: Faster and cheaper while maintaining good quality\n","\n","### Temperature (0.0 - 1.0)\n","- **0.0-0.3**: Deterministic, factual responses\n","- **0.4-0.7**: Balanced creativity and accuracy\n","- **0.8-1.0**: More creative and varied responses\n","\n","### Chunk Size & Overlap\n","- **Chunk Size**: How much text to process at once (256-1024 typical)\n","- **Chunk Overlap**: Overlap between chunks to maintain context (10-100 typical)\n","\n","### Similarity Top-K (1-20)\n","- **Lower values (3-5)**: More focused, faster responses\n","- **Higher values (8-15)**: More comprehensive, detailed responses\n","\n","### Node Postprocessors\n","- **SimilarityPostprocessor**: Filters out low-relevance documents\n","\n","### Similarity Cutoff (0.0-1.0)\n","- **0.1-0.3**: More permissive, includes potentially relevant docs\n","- **0.5-0.8**: More strict, only highly relevant docs\n","\n","### Response Synthesizers\n","- **TreeSummarize**: Hierarchical summarization, good for complex topics\n","- **Refine**: Iterative refinement, builds detailed responses\n","- **CompactAndRefine**: Efficient version of Refine\n","- **Default**: Standard synthesis approach\n"]},{"cell_type":"markdown","metadata":{"id":"KmXe273tS-VL"},"source":["## ‚úÖ Assignment Completion Checklist\n","\n","Before submitting, ensure you have:\n","\n","- [ ] Set up your OPENROUTER_API_KEY environment variable\n","- [ ] Imported all necessary libraries including advanced RAG components\n","- [ ] Created AdvancedRAGBackend class with configurable parameters\n","- [ ] Implemented all required methods:\n","  - [ ] `update_settings()` - Updates LLM and chunking parameters\n","  - [ ] `initialize_database()` - Sets up vector database\n","  - [ ] `get_postprocessor()` - Returns selected postprocessor\n","  - [ ] `get_synthesizer()` - Returns selected synthesizer\n","  - [ ] `advanced_query()` - Handles queries with all configuration options\n","- [ ] Created advanced Gradio interface with all required components:\n","  - [ ] Initialize database button\n","  - [ ] Model selection dropdown (gpt-4o, gpt-4o-mini)\n","  - [ ] Temperature slider (0 to 1, step 0.1)\n","  - [ ] Chunk size input (default 512)\n","  - [ ] Chunk overlap input (default 50)\n","  - [ ] Similarity top-k slider (1 to 20, default 5)\n","  - [ ] Node postprocessor multiselect\n","  - [ ] Similarity cutoff slider (0.0 to 1.0, step 0.1, default 0.3)\n","  - [ ] Response synthesizer dropdown\n","  - [ ] Query input and submit button\n","  - [ ] Response output\n","  - [ ] Configuration display\n","- [ ] Connected all components to backend functions\n","- [ ] Successfully launched the application\n","- [ ] Tested different parameter combinations\n","- [ ] Verified all configuration options work correctly\n","\n","## üéä Congratulations!\n","\n","You've successfully built a professional, production-ready RAG application! You now have:\n","\n","- **Advanced Parameter Control**: Full control over all RAG system parameters\n","- **Professional UI**: Clean, organized interface with proper layout\n","- **Real-time Configuration**: Ability to experiment with different settings\n","- **Production Patterns**: Understanding of how to build scalable AI applications\n","\n","## üöÄ Next Steps & Extensions\n","\n","**Potential Enhancements:**\n","1. **Authentication**: Add user login and session management\n","2. **Document Upload**: Allow users to upload their own documents\n","3. **Chat History**: Implement conversation memory\n","4. **Performance Monitoring**: Add response time and quality metrics\n","5. **A/B Testing**: Compare different configurations side-by-side\n","6. **Export Features**: Download responses and configurations\n","7. **Advanced Visualizations**: Show document similarity scores and retrieval paths\n","\n","**Deployment Options:**\n","- **Local**: Run on your machine for development\n","- **Gradio Cloud**: Deploy with `interface.launch(share=True)`\n","- **Hugging Face Spaces**: Deploy to Hugging Face for public access\n","- **Docker**: Containerize for scalable deployment\n","- **Cloud Platforms**: Deploy to AWS, GCP, or Azure\n","\n","You're now ready to build sophisticated AI-powered applications!\n"]}],"metadata":{"kernelspec":{"display_name":"accelerator","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}