import gradio as gr
from transformers import pipeline
import torch
import json
from datetime import datetime
from gtts import gTTS
import os
import PyPDF2
import docx
from huggingface_hub import HfApi

# ============================================================================
# GLOBAL VARIABLES
# ============================================================================
current_model = None
current_model_name = None

# ============================================================================
# FILE READING FUNCTIONS
# ============================================================================
def read_txt_file(file_path):
    """Read plain text file"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except UnicodeDecodeError:
        with open(file_path, 'r', encoding='latin-1') as f:
            return f.read()

def read_pdf_file(file_path):
    """Read PDF file and extract text"""
    try:
        text = ""
        with open(file_path, 'rb') as f:
            pdf_reader = PyPDF2.PdfReader(f)
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
        return text.strip()
    except Exception as e:
        return f"Error reading PDF: {str(e)}"

def read_docx_file(file_path):
    """Read DOCX file and extract text"""
    try:
        doc = docx.Document(file_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text.strip()
    except Exception as e:
        return f"Error reading DOCX: {str(e)}"

def read_uploaded_file(file):
    """Read uploaded file and extract text based on file type"""
    if file is None:
        return "‚ö†Ô∏è No file uploaded"
    
    file_path = file.name
    file_extension = os.path.splitext(file_path)[1].lower()
    
    try:
        if file_extension == '.txt':
            return read_txt_file(file_path)
        elif file_extension == '.pdf':
            return read_pdf_file(file_path)
        elif file_extension in ['.docx', '.doc']:
            return read_docx_file(file_path)
        else:
            return f"‚ö†Ô∏è Unsupported file format: {file_extension}\nSupported formats: .txt, .pdf, .docx"
    except Exception as e:
        return f"‚ùå Error reading file: {str(e)}"

# ============================================================================
# DYNAMIC MODEL LOADING FROM HUGGING FACE
# ============================================================================
def fetch_top_summarization_models(limit=10):
    """Fetch top summarization models from Hugging Face Hub"""
    try:
        api = HfApi()
        
        # Use filter parameter instead of deprecated task parameter
        models = api.list_models(
            filter="summarization",
            sort="downloads",
            direction=-1,
            limit=limit
        )
        
        model_list = [model.id for model in models]
        
        print(f"‚úÖ Successfully fetched {len(model_list)} models from Hugging Face")
        return model_list
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error fetching models from Hugging Face: {e}")
        print("üìå Using fallback model list")
        
        return [
            "facebook/bart-large-cnn",
            "google/pegasus-xsum",
            "sshleifer/distilbart-cnn-12-6",
            "t5-small",
            "philschmid/bart-large-cnn-samsum",
            "Falconsai/text_summarization",
            "google/pegasus-cnn_dailymail",
            "facebook/bart-large-xsum",
            "lidiya/bart-large-xsum-samsum",
            "knkarthick/MEETING_SUMMARY"
        ]

# Fetch popular models dynamically
print("üîÑ Fetching top summarization models from Hugging Face...")
POPULAR_MODELS = fetch_top_summarization_models(limit=10)
print(f"üìã Loaded models: {', '.join(POPULAR_MODELS[:3])}... (+{len(POPULAR_MODELS)-3} more)")

# ============================================================================
# MODEL LOADING FUNCTION
# ============================================================================
def load_model(model_name, hf_token=None):
    """Load a summarization model from Hugging Face"""
    global current_model, current_model_name

    try:
        if current_model_name == model_name and current_model is not None:
            return f"‚úÖ Model '{model_name}' is already loaded and ready!"

        device = 0 if torch.cuda.is_available() else -1

        if hf_token and hf_token.strip():
            current_model = pipeline(
                "summarization",
                model=model_name,
                token=hf_token,
                device=device
            )
        else:
            current_model = pipeline(
                "summarization",
                model=model_name,
                device=device
            )

        current_model_name = model_name
        device_info = "GPU" if torch.cuda.is_available() else "CPU"
        return f"‚úÖ Model '{model_name}' loaded successfully on {device_info}!"

    except Exception as e:
        return f"‚ùå Error loading model: {str(e)}\n\nTip: Make sure the model name is correct and you have a valid HF token if needed."

# ============================================================================
# SUMMARIZATION FUNCTION
# ============================================================================
def summarize_text(input_text, summary_percentage, min_length):
    """Generate a summary from input text based on compression percentage"""
    global current_model

    if not input_text or not input_text.strip():
        return "‚ö†Ô∏è Please enter text to summarize or upload a file."

    if current_model is None:
        return "‚ö†Ô∏è Please load a model first!"

    try:
        input_char_count = len(input_text)
        estimated_tokens = input_char_count // 4
        max_length = int(estimated_tokens * (summary_percentage / 100))
        
        max_length = max(30, max_length)
        if max_length <= min_length:
            max_length = min_length + 20

        max_input_length = 1024
        if estimated_tokens > max_input_length:
            words = input_text.split()
            chunk_size = max_input_length * 4
            chunks = []
            current_chunk = []
            current_length = 0
            
            for word in words:
                current_length += len(word) + 1
                if current_length > chunk_size:
                    chunks.append(' '.join(current_chunk))
                    current_chunk = [word]
                    current_length = len(word)
                else:
                    current_chunk.append(word)
            
            if current_chunk:
                chunks.append(' '.join(current_chunk))
            
            summaries = []
            for chunk in chunks:
                result = current_model(
                    chunk,
                    max_length=int(max_length // len(chunks)),
                    min_length=int(min_length // len(chunks)),
                    do_sample=False
                )
                summaries.append(result[0]['summary_text'])
            
            summary = ' '.join(summaries)
        else:
            result = current_model(
                input_text,
                max_length=int(max_length),
                min_length=int(min_length),
                do_sample=False
            )
            summary = result[0]['summary_text']

        output = f"üìù **Summary Generated Successfully!**\n\n"
        output += f"**Original Length:** {len(input_text)} characters ({len(input_text.split())} words)\n"
        output += f"**Summary Length:** {len(summary)} characters ({len(summary.split())} words)\n"
        output += f"**Compression Ratio:** {(1 - len(summary)/len(input_text))*100:.1f}%\n\n"
        output += f"---\n\n{summary}"

        return output

    except Exception as e:
        return f"‚ùå Error generating summary: {str(e)}"

# ============================================================================
# TEXT-TO-SPEECH FUNCTION
# ============================================================================
def text_to_speech(summary_output):
    """Convert summary text to speech audio file"""
    if not summary_output or "‚ö†Ô∏è" in summary_output or "‚ùå" in summary_output:
        return None
    
    try:
        summary_text = summary_output.split('---')[-1].strip() if '---' in summary_output else summary_output
        
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        filename = f"summary_audio_{timestamp}.mp3"
        
        tts = gTTS(text=summary_text, lang='en', slow=False)
        tts.save(filename)
        
        return filename
    
    except Exception as e:
        print(f"Error generating speech: {str(e)}")
        return None

# ============================================================================
# EXPORT FUNCTIONS
# ============================================================================
def export_txt(input_text, summary_output):
    """Export summary as a formatted TXT file"""
    if not summary_output or "‚ö†Ô∏è" in summary_output or "‚ùå" in summary_output:
        return None

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"summary_{timestamp}.txt"

    content = f"Text Summary Report\n"
    content += f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    content += f"Model: {current_model_name}\n"
    content += f"\n{'='*60}\n\n"
    content += f"ORIGINAL TEXT:\n{input_text}\n\n"
    content += f"{'='*60}\n\n"
    content += f"SUMMARY:\n{summary_output.split('---')[-1].strip() if '---' in summary_output else summary_output}\n"

    with open(filename, 'w', encoding='utf-8') as f:
        f.write(content)

    return filename

def export_json(input_text, summary_output):
    """Export summary as a structured JSON file"""
    if not summary_output or "‚ö†Ô∏è" in summary_output or "‚ùå" in summary_output:
        return None

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"summary_{timestamp}.json"

    summary_text = summary_output.split('---')[-1].strip() if '---' in summary_output else summary_output

    data = {
        "timestamp": datetime.now().isoformat(),
        "model": current_model_name,
        "original_text": input_text,
        "summary": summary_text,
        "original_length": len(input_text),
        "summary_length": len(summary_text),
        "compression_ratio": f"{(1 - len(summary_text)/len(input_text))*100:.1f}%"
    }

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

    return filename

def export_summary_only_txt(summary_output):
    """Export only the summary text (without original) as TXT"""
    if not summary_output or "‚ö†Ô∏è" in summary_output or "‚ùå" in summary_output:
        return None

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"summary_only_{timestamp}.txt"
    
    summary_text = summary_output.split('---')[-1].strip() if '---' in summary_output else summary_output

    with open(filename, 'w', encoding='utf-8') as f:
        f.write(summary_text)

    return filename

# ============================================================================
# MODEL INFO DISPLAY FUNCTION
# ============================================================================
def update_model_info(model_name):
    """Update model description based on selected model"""
    model_descriptions = {
        "facebook/bart-large-cnn": """
---
### üìñ Model Information:
**facebook/bart-large-cnn**
- üéØ **Best for**: News articles, formal content, general text
- ‚ö° **Speed**: Medium (2-4 seconds)
- üé® **Style**: Abstractive summarization
- üìä **Quality**: High
- üíæ **Size**: ~1.6 GB
        """,
        
        "google/pegasus-xsum": """
---
### üìñ Model Information:
**google/pegasus-xsum**
- üéØ **Best for**: Extreme compression, one-sentence summaries
- ‚ö° **Speed**: Medium (2-4 seconds)
- üé® **Style**: Highly abstractive, concise
- üìä **Quality**: Very High
- üíæ **Size**: ~2.3 GB
        """,
        
        "sshleifer/distilbart-cnn-12-6": """
---
### üìñ Model Information:
**sshleifer/distilbart-cnn-12-6**
- üéØ **Best for**: Fast processing, general use
- ‚ö° **Speed**: Fast (1-2 seconds)
- üé® **Style**: Abstractive, efficient
- üìä **Quality**: Good
- üíæ **Size**: ~400 MB (distilled version)
        """,
        
        "t5-small": """
---
### üìñ Model Information:
**t5-small**
- üéØ **Best for**: General purpose, varied content
- ‚ö° **Speed**: Very Fast (<1 second)
- üé® **Style**: Extractive-abstractive hybrid
- üìä **Quality**: Good
- üíæ **Size**: ~240 MB (lightweight)
        """,
    }
    
    if model_name in model_descriptions:
        return model_descriptions[model_name]
    else:
        return f"""
---
### üìñ Model Information:
**{model_name}** (Custom/Dynamic Model)
- üéØ Fetched from Hugging Face Hub
- ‚ö° Speed and quality depend on model architecture
- üí° Make sure it's a summarization model

### üîë Get HF Token:
Visit [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
        """

# ============================================================================
# SAMPLE TEXT LOADER
# ============================================================================
def load_sample_text():
    """Load sample text about the Eiffel Tower for testing"""
    return """The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Locally nicknamed "La dame de fer" (French for "Iron Lady"), it was constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair. The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."""

def clear_all():
    """Clear all input and output fields"""
    return "", "", None, None, None

# ============================================================================
# THEME CONFIGURATION
# ============================================================================
dark_theme = gr.themes.Base(
    primary_hue="blue",
    secondary_hue="slate",
    neutral_hue="slate",
).set(
    body_background_fill="*neutral_950",
    body_background_fill_dark="*neutral_950",
    body_text_color="*neutral_100",
    body_text_color_subdued="*neutral_400",
    
    background_fill_primary="*neutral_900",
    background_fill_primary_dark="*neutral_900",
    background_fill_secondary="*neutral_800",
    background_fill_secondary_dark="*neutral_800",
    
    border_color_primary="*neutral_700",
    border_color_primary_dark="*neutral_700",
    color_accent_soft="*primary_500",
    
    block_background_fill="*neutral_900",
    block_background_fill_dark="*neutral_900",
    block_border_width="1px",
    block_label_background_fill="*neutral_800",
    block_label_background_fill_dark="*neutral_800",
    block_label_text_color="*neutral_100",
    block_label_text_color_dark="*neutral_100",
    block_title_text_color="*neutral_50",
    block_title_text_color_dark="*neutral_50",
    block_info_text_color="*neutral_300",
    block_info_text_color_dark="*neutral_300",
    
    input_background_fill="*neutral_800",
    input_background_fill_dark="*neutral_800",
    input_border_color="*neutral_600",
    input_border_color_dark="*neutral_600",
    input_placeholder_color="*neutral_400",
    
    button_primary_background_fill="*primary_600",
    button_primary_background_fill_dark="*primary_600",
    button_primary_background_fill_hover="*primary_700",
    button_primary_background_fill_hover_dark="*primary_700",
    button_primary_text_color="white",
    button_primary_text_color_dark="white",
)

# ============================================================================
# CUSTOM CSS FOR RESPONSIVENESS
# ============================================================================
custom_css = """
.gradio-container {
    max-width: 100% !important;
    padding: 1rem !important;
}

.block-label, label, .label {
    color: #f3f4f6 !important;
}

.block-info, .info {
    color: #d1d5db !important;
}

h1, h2, h3, h4, h5, h6 {
    color: #f9fafb !important;
}

input, textarea {
    color: #f3f4f6 !important;
    background-color: #1f2937 !important;
    border: 1px solid #4b5563 !important;
}

input::placeholder, textarea::placeholder {
    color: #9ca3af !important;
}

.dropdown-label, .dropdown-arrow {
    color: #f3f4f6 !important;
}

@media (max-width: 768px) {
    .gradio-container {
        padding: 0.5rem !important;
    }
    
    .wrap {
        gap: 0.5rem !important;
    }
    
    .form {
        gap: 0.5rem !important;
    }
}

.primary-btn {
    width: 100%;
}

.markdown {
    color: #e5e7eb !important;
}

.markdown h1, .markdown h2, .markdown h3 {
    color: #f3f4f6 !important;
}

.markdown code {
    background-color: #1f2937;
    padding: 0.2rem 0.4rem;
    border-radius: 0.25rem;
    color: #f3f4f6 !important;
}

.gr-slider label {
    color: #f3f4f6 !important;
}

span, p, div {
    color: #e5e7eb;
}

.file-preview {
    background-color: #1f2937 !important;
    border: 1px solid #4b5563 !important;
}
"""

# ============================================================================
# GRADIO INTERFACE
# ============================================================================
with gr.Blocks(theme=dark_theme, css=custom_css, title="Text Summarization Tool") as demo:

    gr.Markdown("""
    # üìÑ Advanced Text Summarization Tool
    ### Powered by Hugging Face Transformers üéôÔ∏è with Text-to-Speech
    
    Supports **TXT**, **PDF**, and **DOCX** files | Copy-paste or upload | Export in multiple formats
    """)

    with gr.Row():
        with gr.Column(scale=1, min_width=300):
            gr.Markdown("## üîß Model Configuration")

            with gr.Group():
                model_dropdown = gr.Dropdown(
                    choices=POPULAR_MODELS,
                    value=POPULAR_MODELS[0],
                    label="Select Popular Model",
                    info="Choose from top Hugging Face models"
                )

                custom_model = gr.Textbox(
                    label="Or Enter Custom Model",
                    placeholder="e.g., user/custom-model-name",
                    info="Any Hugging Face summarization model"
                )

                hf_token = gr.Textbox(
                    label="Hugging Face Token (Optional)",
                    placeholder="hf_...",
                    type="password",
                    info="Required for private models or to avoid rate limits"
                )

                load_btn = gr.Button("üöÄ Load Model", variant="primary", size="lg")
                model_status = gr.Textbox(
                    label="Model Status",
                    interactive=False,
                    lines=2
                )

            model_info = gr.Markdown(update_model_info(POPULAR_MODELS[0]))

        with gr.Column(scale=2, min_width=400):
            gr.Markdown("## üìù Input & Summarization")

            with gr.Group():
                gr.Markdown("### üì• Input Methods")
                
                with gr.Tabs():
                    with gr.Tab("‚úçÔ∏è Text Input"):
                        input_text = gr.Textbox(
                            label="Enter or Paste Text",
                            placeholder="Type or paste your text here...",
                            lines=10,
                            max_lines=20
                        )
                        
                        with gr.Row():
                            sample_btn = gr.Button("üìã Load Sample", size="sm")
                            clear_btn = gr.Button("üóëÔ∏è Clear All", size="sm", variant="stop")
                    
                    with gr.Tab("üìÅ File Upload"):
                        file_upload = gr.File(
                            label="Upload File",
                            file_types=[".txt", ".pdf", ".docx", ".doc"],
                            type="filepath"
                        )
                        upload_btn = gr.Button("üì§ Load from File", variant="secondary", size="sm")
                        
                        gr.Markdown("""
                        **Supported formats:**
                        - üìÑ Plain text (.txt)
                        - üìï PDF documents (.pdf)
                        - üìò Word documents (.docx, .doc)
                        """)

                gr.Markdown("### ‚öôÔ∏è Summarization Settings")
                with gr.Row():
                    summary_percentage = gr.Slider(
                        minimum=10,
                        maximum=90,
                        value=30,
                        step=5,
                        label="Summary Length (%)",
                        info="Target summary as % of original length"
                    )
                    min_length = gr.Slider(
                        minimum=10,
                        maximum=100,
                        value=30,
                        step=5,
                        label="Min Summary Length",
                        info="Minimum number of tokens"
                    )

                summarize_btn = gr.Button("‚ú® Generate Summary", variant="primary", size="lg")

            with gr.Group():
                gr.Markdown("### üìä Summary Output")
                summary_output = gr.Textbox(
                    label="Generated Summary",
                    lines=8,
                    max_lines=20,
                    interactive=False,
                    show_copy_button=True
                )

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### üéôÔ∏è Audio")
                        tts_btn = gr.Button("üîä Generate Audio", size="sm", variant="secondary")
                        audio_output = gr.Audio(label="Summary Audio", type="filepath")
                    
                    with gr.Column(scale=1):
                        gr.Markdown("#### üíæ Export")
                        with gr.Row():
                            export_txt_btn = gr.Button("üìÑ Full Report (TXT)", size="sm")
                            export_summary_btn = gr.Button("üìù Summary Only (TXT)", size="sm")
                        export_json_btn = gr.Button("üì¶ JSON Export", size="sm")
                        
                        with gr.Row():
                            txt_file = gr.File(label="Download TXT", visible=True)
                            json_file = gr.File(label="Download JSON", visible=True)

    gr.Markdown("""
    ---
    ### üí° Pro Tips:
    
    **üì• Input Options:**
    - ‚úçÔ∏è **Copy-Paste**: Directly paste text into the input box
    - üìÅ **File Upload**: Drag & drop or browse TXT, PDF, or DOCX files
    - üìã **Sample Text**: Try with pre-loaded example
    
    **‚öôÔ∏è Optimization:**
    - üìä **Summary %**: Lower percentage = shorter summary (10-30% recommended)
    - üöÄ **Model Selection**: Start with smaller models for speed
    - üéØ **Long Documents**: App automatically chunks long texts
    
    **üíæ Export Options:**
    - üìÑ **Full Report**: Original + summary + metadata
    - üìù **Summary Only**: Just the summary text
    - üì¶ **JSON Format**: Structured data
    - üîä **Audio**: Listen to summaries
    """)

    # Event handlers
    def update_model_name(dropdown_value, custom_value):
        return custom_value if custom_value.strip() else dropdown_value

    model_dropdown.change(
        fn=update_model_info,
        inputs=[model_dropdown],
        outputs=model_info
    )
    
    custom_model.change(
        fn=lambda custom: update_model_info(custom) if custom.strip() else update_model_info(POPULAR_MODELS[0]),
        inputs=[custom_model],
        outputs=model_info
    )

    load_btn.click(
        fn=lambda d, c, t: load_model(update_model_name(d, c), t),
        inputs=[model_dropdown, custom_model, hf_token],
        outputs=model_status
    )

    sample_btn.click(
        fn=load_sample_text,
        outputs=input_text
    )

    upload_btn.click(
        fn=read_uploaded_file,
        inputs=[file_upload],
        outputs=input_text
    )

    clear_btn.click(
        fn=clear_all,
        outputs=[input_text, summary_output, audio_output, txt_file, json_file]
    )

    summarize_btn.click(
        fn=summarize_text,
        inputs=[input_text, summary_percentage, min_length],
        outputs=summary_output
    )

    tts_btn.click(
        fn=text_to_speech,
        inputs=[summary_output],
        outputs=audio_output
    )

    export_txt_btn.click(
        fn=export_txt,
        inputs=[input_text, summary_output],
        outputs=txt_file
    )

    export_summary_btn.click(
        fn=export_summary_only_txt,
        inputs=[summary_output],
        outputs=txt_file
    )

    export_json_btn.click(
        fn=export_json,
        inputs=[input_text, summary_output],
        outputs=json_file
    )

if __name__ == "__main__":
    demo.launch(share=True)
