{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMH3BEaIbSEiosMucrB+Fw9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eng-accelerator/Submissions_C2/blob/main/Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB6_9rcKfuWP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRADIO SYNTAX"
      ],
      "metadata": {
        "id": "qBdN3hkkfH6R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR4y7UudewDw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "2ec8b795-5b10-462c-f91b-14943e6712e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://35485350369c1bf6d6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://35485350369c1bf6d6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(name):\n",
        "    return \"Hello, \" + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "\n",
        "# To create a shareable link (valid for 72 hours)\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HUGGING FACE TOKEN LOGISTICS"
      ],
      "metadata": {
        "id": "53iZXyE0fFTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import whoami\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your Hugging Face token from Colab Secrets\n",
        "hf_token = userdata.get('HF')\n",
        "\n",
        "# Verify the token by checking your identity\n",
        "try:\n",
        "    user_info = whoami(token=hf_token)\n",
        "    print(f\"Logged in as: {user_info['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not log in: {e}\")\n",
        "    print(\"Please make sure you have added your Hugging Face token to Colab Secrets with the name 'HF_TOKEN'\")"
      ],
      "metadata": {
        "id": "BvuK-FRUe4sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6002455d-cdb5-4b0a-abd2-ddd563c54848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.10.5)\n",
            "Logged in as: neelaym\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IF YOU WISH TO LOAD SOME DATASET TO TEST ANYTHING"
      ],
      "metadata": {
        "id": "SJ7RyQEXfKqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load a dataset (e.g., the SQuAD dataset for question answering)\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Print information about the dataset\n",
        "print(dataset)\n",
        "\n",
        "# Access an example from the training set\n",
        "print(\"\\nExample from the training set:\")\n",
        "print(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "kKofrkg0fCEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAMPLE SUMMARISATION CODE"
      ],
      "metadata": {
        "id": "ixTksrfyfNwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "# Load the summarization pipeline\n",
        "\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
        "\n",
        "\n",
        "# Text to summarize\n",
        "text = \"\"\"\n",
        "Hugging Face is a company and open-source platform that provides tools and models for natural language processing (NLP). It has become a central hub for the ML community, offering a wide range of pre-trained models that can be easily used or fine-tuned for specific applications. Key aspects of Hugging Face include the Transformers library, Model Hub, Datasets library, and Tokenizers library. Hugging Face democratizes access to powerful ML models, making it easier for developers and researchers to build and deploy applications.\n",
        "\"\"\"\n",
        "\n",
        "# Summarize the text\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "\n",
        "def summarize_text(text):\n",
        "    summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "\n",
        "\n",
        "demo = gr.Interface(fn=summarize_text, inputs=\"text\", outputs=\"text\", theme=\"gradio/dracula\")\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nSummary:\")\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "x6UgM-Rse5dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1793c0a6-4f92-4370-be82-dee9bf9329cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "\n",
            "Hugging Face is a company and open-source platform that provides tools and models for natural language processing (NLP). It has become a central hub for the ML community, offering a wide range of pre-trained models that can be easily used or fine-tuned for specific applications. Key aspects of Hugging Face include the Transformers library, Model Hub, Datasets library, and Tokenizers library. Hugging Face democratizes access to powerful ML models, making it easier for developers and researchers to build and deploy applications.\n",
            "\n",
            "\n",
            "Summary:\n",
            "Hugging Face is a company and open-source platform that provides tools and models for natural language processing (NLP). It has become a central hub for the ML community, offering a wide range of pre-trained models that can be easily used or fine-tuned for specific applications .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/blocks.py:1069: UserWarning: Cannot load gradio/dracula. Caught Exception: The space gradio/dracula does not exist\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ASSIGNMENT GOES BELOW -"
      ],
      "metadata": {
        "id": "2uRw9YNZfP13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
        "\n",
        "# Summarize text\n",
        "def summarize_text(text):\n",
        "    if not text.strip():\n",
        "        return \"Please enter some text to summarize.\"\n",
        "    summary = summarizer(text, max_length=120, min_length=30, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Export summary to a downloadable file\n",
        "def export_summary(summary):\n",
        "    file_path = \"summary.txt\"\n",
        "    with open(file_path, \"w\") as f:\n",
        "        f.write(summary)\n",
        "    return file_path\n",
        "\n",
        "# UI layout\n",
        "def build_app(theme=\"gradio/soft\"):\n",
        "    with gr.Blocks(theme=theme) as demo:\n",
        "        gr.Markdown(\"## ðŸ§  AI Text Summarizer\")\n",
        "        gr.Markdown(\"Enter any long paragraph below to generate a concise summary.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            theme_toggle = gr.Radio(\n",
        "                [\"gradio/soft\", \"gradio/dracula\"],\n",
        "                value=theme,\n",
        "                label=\"ðŸŒ“ Theme\"\n",
        "            )\n",
        "\n",
        "        text_input = gr.Textbox(\n",
        "            lines=10,\n",
        "            placeholder=\"Paste your text here...\",\n",
        "            label=\"Input Text\"\n",
        "        )\n",
        "\n",
        "        summarize_btn = gr.Button(\"âœ¨ Summarize\")\n",
        "        output = gr.Textbox(label=\"Summary\", lines=8)\n",
        "        export_btn = gr.Button(\"ðŸ“„ Export Summary\")\n",
        "        download_file = gr.File(label=\"Download Summary\")\n",
        "\n",
        "        # Events\n",
        "        summarize_btn.click(summarize_text, inputs=text_input, outputs=output)\n",
        "        export_btn.click(export_summary, inputs=output, outputs=download_file)\n",
        "\n",
        "        # Rebuild app when theme changes\n",
        "        def switch_theme(selected_theme):\n",
        "            return gr.update(theme=selected_theme)\n",
        "\n",
        "        theme_toggle.change(lambda t: None, inputs=theme_toggle, outputs=None)  # placeholder\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "# Launch app\n",
        "demo = build_app()\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "bCAb092HfRrj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "ba23f9b8-7f50-4285-97d5-44e9fbc54c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b4dfd4fdee179c6335.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b4dfd4fdee179c6335.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}